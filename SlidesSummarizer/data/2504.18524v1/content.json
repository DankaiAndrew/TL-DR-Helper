{
  "title": "2504.18524v1",
  "pages": [
    {
      "page_num": 1,
      "text": "Augmenting Perceptual Super-Resolution via Image Quality Predictors\nFengjia Zhang*\nSamrudhdhi B. Rangrej*\nTristan Aumentado-Armstrong*\nAfsaneh Fazly\nAlex Levinshtein\nAI Center – Toronto, Samsung Electronics\n{f.zhang2, s.rangrej, tristan.a, a.fazly, alex.lev}@samsung.com\nDifferentiable\nFast\nMultimodal Supervision\n=\n>\n>\nRanks\nFast\nPrecise\nHuman derived\nIQA derived\n>\n>\n>\nImage Quality Optimization\nscore = 0.87\nscore = 0.75\nscore = 0.57\nscore = 0.43\nSR\nModel\nGround-truth\nPreference?\nRanks\nFast\nPrecise\nEnhanced ground-truths\nIQA \nMetric\nScore\nSR\nModel\nGood\nBad\nOptimization\nmetric?\nDifferentiable\nFast\nLR image\nSR image\nLR image\nSR image\nSR image\nSR image\nEGT 01\nEGT 02\nEGT 03\nEGT 04\nEGT 03\nEGT 02\nEGT 01\nEGT 04\nEGT 03\nEGT 02\nEGT 01\nEGT 04\nEGT i\nHuman Score\nIQA Score\nIQA \nMetric\nScore\nFigure 1. Schematics for improving perceptual super-resolution (SR). Perceptual quality of SR can be improved in two ways: (Left)\nproviding supervision through multiple enhanced ground-truths (EGT) or (Right) direct optimization for the quality of the super-resolved\nimage. In both cases, human-in-the-loop can greatly improve performance. However, manual annotation is tedious, imprecise, and non-\ndifferentiable. An IQA metric can replace a human in rating the enhanced ground-truths or can directly act as a differentiable optimization\nobjective. In this paper, we specifically assess whether more practical no-reference (NR) IQA metrics can replace human raters for SR. We\nfind that combining NR-IQA-based sampling and regularized optimization is sufficient to attain state-of-the-art perceptual image quality,\nwithout requiring human ratings.\nAbstract\nSuper-resolution (SR), a classical inverse problem in com-\nputer vision, is inherently ill-posed, inducing a distribution\nof plausible solutions for every input.\nHowever, the de-\nsired result is not simply the expectation of this distribution,\nwhich is the blurry image obtained by minimizing pixelwise\nerror, but rather the sample with the highest image quality.\nA variety of techniques, from perceptual metrics to adver-\nsarial losses, are employed to this end. In this work, we\nexplore an alternative: utilizing powerful non-reference im-\nage quality assessment (NR-IQA) models in the SR context.\nWe begin with a comprehensive analysis of NR-IQA met-\nrics on human-derived SR data, identifying both the accu-\nracy (human alignment) and complementarity of different\nmetrics. Then, we explore two methods of applying NR-\nIQA models to SR learning: (i) altering data sampling, by\nbuilding on an existing multi-ground-truth SR framework,\nand (ii) directly optimizing a differentiable quality score.\nOur results demonstrate a more human-centric perception-\ndistortion tradeoff, focusing less on non-perceptual pixel-\nwise distortion, instead improving the balance between per-\nceptual fidelity and human-tuned NR-IQA measures.\n1. Introduction\nMany tasks in human and computer vision are naturally for-\nmulated as ill-posed inverse problems [65]. Single-image\nsuper-resolution (SISR), which has many practical applica-\ntions to digital photographic zoom, is a well-studied exam-\nple of this (e.g., [4, 24]). In SISR, a given low-resolution\n(LR) image has an associated distribution of high-resolution\n(HR) “real” images that could have given rise to it. The fun-\ndamental challenge of SR is therefore not just to find any\nsample from that distribution, but instead to find perceptu-\nally plausible one(s). Early learning-based models, trained\nwith pixel-wise losses (e.g., [21, 22]), effectively “aver-\nage” over possible solutions in pixel-space, resulting in\n*Equal Contribution.\n1\narXiv:2504.18524v1  [cs.CV]  25 Apr 2025\n"
    },
    {
      "page_num": 2,
      "text": "blurry output images with a high peak signal-to-noise ratio\n(PSNR). However, human preferences indicate that a solu-\ntion with high image quality is better than this averaged one.\nHence, numerous techniques have been devised to empha-\nsize perceptual fidelity, such as perceptual metrics [19, 39]\nand adversarial losses (e.g., [70]), greatly improving image\nquality. In other words, pixelwise fidelity is a poor measure\nof perceptual quality. In fact, under some conditions, they\nare directly oppositional, forming a “perception–distortion\ntradeoff” [5, 6]. In theory, the only pixel-space constraint is\ngiven by the LR image; besides this, the optimal SR result\n(in terms of human preference) may have very high pix-\nelwise distortion (w.r.t. the “real” ground-truth generating\nimage), as long as it has high plausibility with respect to the\nLR input and high image quality.\nInstead of optimizing pixel-space distortions, we focus\non improving perceptual image quality. This is commonly\ndone using a combination of perceptual losses and GANs\n[48, 58, 61, 79, 84], enabling the SR model to target a\nmulti-modal distribution rather than specific ground-truth\ntargets.\nThe challenge of such methods, however, is to\nproduce perceptually plausible outputs without introduc-\ning high-frequency artifacts [51]. Many full-reference (FR)\n[18, 27, 42, 99] and non-reference image quality assessment\n(NR-IQA) [41, 68, 74, 75] metrics were developed to align\nwith human preferences for identifying perceptually plausi-\nble images. While some approaches started to replace per-\nceptual losses with FR metrics like LPIPS [42] and DISTS\n[18], for the task of image restoration, NR-IQA metrics are\nstill used purely for evaluation purposes. Motivated by the\nadoption of human feedback guidance in text-to-image gen-\nerative models (e.g., [16, 23, 67, 92]), we aim to use NR-\nIQA metrics to improve SISR.\nRecently, Chen et al. [12] used human feedback to im-\nprove SISR. They do so by generating multiple enhanced\nversions of GT (Fig. 1 left), manually rating these different\nversions using multiple human evaluators, and fine-tuning\nthe model on the positively ranked GTs.\nWhile this re-\nsults in significant perceptual quality improvements with-\nout introducing unwanted artifacts, manual human ranking\nis very coarse and cumbersome. We instead use an auto-\nmatic NR-IQA measure that is well-correlated with human\nscores, yielding a more fine-grained ranking, and bypassing\nthe requirement for having human feedback (Fig. 1 centre).\nAdditionally, since the measure is fully differentiable, it can\nbe used for direct optimization (e.g., replacing or comple-\nmenting GANs), unlike human scores that cannot be used in\nthis fashion (Fig. 1 right). Our contributions are as follows:\n• We present a detailed analysis of NR-IQA on two human-\nderived SR datasets, thus identifying metrics that are gen-\nerally useful for improving SR image quality.\n• We explore the application of NR-IQA to SISR, via two\napproaches: sampling across multiple GTs weighted by\nNR-IQA, and direct optimization of NR-IQA.\n• We achieve SISR results that are perceptually on par or\nbetter than SISR finetuned with human feedback, but us-\ning an automatic NR-IQA measure instead.\n2. Related Work\nDeep learning-based SISR: Deep learning has given a sig-\nnificant boost to SISR performance, taking the SOTA man-\ntle from dictionary-based methods [76, 77, 93] to CNN-\nbased approaches like SRNet and its successors [2, 21, 22,\n44]. Since then, there came many architectural improve-\nments: deeper architectures, like RCAN [102], hierarchical\nprocessing [47], advanced building blocks like in NAFNet\n[13] and RRDB from ESRGAN [84], and better upsampling\nlike PixelUnshuffle [71], to name a few. Due to long range\ndependencies in SISR, transformer-based methods [14, 50],\nand auto-regressive models based on Mamba [54], have re-\ncently achieved SOTA performance [15, 32].\nPerceptual\nquality-oriented\nSISR:\nEarly\nSISR\nap-\nproaches optimized a simple pixel-wise reconstruction loss,\nsuch as L2 or L1, between model output and ground-truth\n[21, 22]. However, due to the ill-posedness of SISR, this\nyields poor perceptual quality. Blau and Michaeli [6] have\nshown that there exists a tradeoff between good perceptual\nquality and accurate reconstruction (or fidelity). To improve\nperceptual quality, perceptual losses were proposed, such as\nSSIM [86], which measures patch similarity rather than per-\npixel similarity, and later others [39] that measure the simi-\nlarity between deep VGG features rather than pixel intensi-\nties. Combined with perceptual losses, GAN-based training\n[30] was used to improve perceptual quality in SRGAN [48]\nand many follow-up approaches [3, 58, 61, 79, 84]. One\nchallenge of such approaches is to improve perceptual qual-\nity without introducing unwanted hallucinations. Proposed\nsolutions include more specialized discriminators [62] and\nbetter balancing between various loss terms [51, 63]. Rather\nthan changing the training loss function, perceptual qual-\nity can be improved by explicitly generating multiple train-\ning targets [12, 38], or encouraged using specific architec-\ntural designs. Normalizing flows [56, 96] were used to di-\nrectly output a distribution over plausible solutions rather\nthan a single SR image. More recently, diffusion-based ap-\nproaches [40, 69, 83, 89–91, 95] and alike [17] have been\nshown to achieve a better perceptual quality than GAN-\nbased methods.\nHuman guided perceptual quality assessment: One may\nuse image quality assessment (IQA) metrics to evaluate the\naesthetic quality of super-resolved images.\nThey can be\nfull-reference (FR) or no-reference (NR) metrics. In real-\nworld applications where the true HR image is unavailable,\nNR-IQA metrics are more useful. Early opinion-unaware\nNR-IQA metrics used hand-crafted features to assess how\nclosely the statistics of the output images match with nat-\n2\n"
    },
    {
      "page_num": 3,
      "text": "Method\nPaQ-2-PiQ [97]\nNIMA† [74]\nMUSIQ◁[41]\nLIQE♡[101]\nARNIQA-TID⋆[1]\nQ-Align⋄[88]\nTOPIQ-NR [11]\nAccuracy (%)\n76.41\n74.91\n74.47\n74.03\n74.03\n73.77\n73.06\nTable 1. Phase I analysis on SBS180K. Accuracy of top 7 NR-IQA metrics on a subset (1212 image pairs) from train portion. We use the\ndefault configuration of the metrics, provided by the IQA-PyTorch toolbox, unless stated otherwise. †We use NIMA with Inception V2\nas base model. ◁We use the default MUSIQ trained on KonIQ [35]. ♡We use LIQE pretrained on KonIQ [35]. ⋆We use ARNIQA metric\ntrained on TID2013 [66]. ⋄We use Q-Align metric specialized in image quality assessment.\nMethod\nPaQ-2-PiQ [97]\nNIMA† [74]\nMUSIQ◁[41]\nLIQE♡[101]\nARNIQA-TID⋆[1]\nQ-Align⋄[88]\nTOPIQ-NR [11]\nTrain Acc. (%)\n80.41\n79.32\n79.96\n77.70\n77.74\n80.00\n78.30\nTest Acc. (%)\n80.57\n81.37\n82.73\n77.45\n77.07\n80.68\n81.28\nTable 2. Phase II analysis on SBS180K dataset. Accuracy of top 7 (according to Phase I) NR-IQA metrics on the entire train and test\nsets of SBS180K. We exclude image pairs where humans prefer both images equally. †◁♡⋆⋄See Table 1.\nural scene statistics, e.g., BRISQUE [59] and NIQE [98].\nOthers developed metrics that align with human preferences\n[68, 75]. Recently, many developed deep-learning-based\nopinion-unaware approaches such as FID [34], and opinion-\naware approaches such as NIMA [74] and MUSIQ [41].\nWhile the above metrics are general purpose, metrics\nsuch as NRQM [57] and NeuralSBS [43] are specifi-\ncally designed to align with human preferences for super-\nresolution. Recently, human preferences are being widely\nincorporated in improving generative models, especially for\ntext-to-image generation [16, 52, 92]. Yet, incorporating\nhuman guidance in improving SR models has not received\nmuch attention. Ding et al. [19] attempt to use various full-\nreference IQA metrics for SR model finetuning, and Chen\net al. [12] propose to use human guidance in GT selection\nprocess. Unlike these works, we attempt to assess the abil-\nity of no-reference IQA metrics to (a) automatically rate and\nselect optimal GT, eliminating the need for arduous manual\nannotation, and (b) act as a finetuning objective to improve\naesthetic quality of super-resolved images.\n3. Analysis of NR-IQA metrics\nWe analyze the alignment between various NR-IQA metrics\nand human judgements for assessing the aesthetic quality\nof super-resolved images. We report our analysis on two\npublicly available datasets, SBS180K [43] and HGGT [12].\n3.1. Analysis on the SBS180K dataset\nWe analyze various NR-IQA metrics on SBS180K [43], a\nlarge scale human preference dataset for super-resolved im-\nages, containing 167,019 train and 9,421 test image pairs.\nEach pair is annotated with a single score in the range [0,1],\ndepicting the fraction of human annotators preferring the\naesthetic quality of the second image over the first one.\nEach pair consists of two super-resolved versions of the\nsame low-resolution image, with each version generated us-\ning a different SR model. Due to the large size of SBS180K,\nwe analyze the metrics in two phases. In phase I, we ana-\nlyze 20 NR-IQA metrics and their variants (total 42 metrics)\non a small subset of the train set. In phase II, we analyze the\ntop 7 NR-IQA metrics from phase I on the complete train\nand test sets. We use IQA-PyTorch[10], an open-source\ntoolbox for image quality assessment.\nPhase I. There are 404 unique pairs of compared SR models\nin the train set. We randomly select 3 image pairs per model\ncomparison, yielding a subset of 1212 images. We evaluate\nthe following NR-IQA metrics on this subset: Q-Align [88],\nLIQE [101], ARNIQA [1], TOPIQ [11], TReS [29], CLIP-\nIQA(+) [82], MANIQA [94], MUSIQ [41], DBCNN [100],\nPaQ-2-PiQ [97], HyperIQA [72], NIMA [74], WaDIQaM\n[8], CNNIQA [104], NRQM [57], PI (Perceptual Index)\n[7], BRISQUE [59], ILNIQE and NIQE [98], and PIQE\n[81]. When available, we also consider multiple variants\nof these metrics offered by IQA-PyTorch (e.g., metrics\ntrained on different IQA datasets). We assess the accuracy\nof each metric in terms of whether, given an image-pair,\nthe metric prefers the same image as the humans prefer col-\nlectively or not. Table 1 shows results of only the top 7\nmetrics: PaQ-2-PiQ, NIMA, MUSIQ, LIQE, ARNIQA, Q-\nAlign, TOPIQ-NR. Complete results are given in Supp. Ta-\nble 6.\nPhase II. We evaluate the top 7 metrics from Phase I on\nthe complete train and test sets, excluding pairs with no\nconsensus among human annotators (score of 0.5). Results\nare given in Table 2, suggesting that MUSIQ has relatively\nhigher accuracy on both train and test sets compared to other\nmetrics. While PaQ-2-PiQ and Q-Align have slightly higher\naccuracy (0.45% and 0.04%, respectively) on the train set,\nMUSIQ outperforms them on the test set by a large mar-\ngin (2.16% and 2.05%, respectively). We further analyze\nperformance of the remaining six metrics on the samples\nwhere MUSIQ fails (excluding pairs with a score of 0.5).\nResults are shown in Table 3. We find that NIMA and Q-\nAlign achieve higher accuracy on these samples compared\n3\n"
    },
    {
      "page_num": 4,
      "text": "to other four metrics. Since they complement MUSIQ, in\n§5, we report performance on MUSIQ, NIMA and Q-Align.\n3.2. Analysis on HGGT dataset\nWe analyze the seven selected metrics of Phase I above on\nthe HGGT [12] dataset, containing 20,193 quintuplets of\nHR image patches. Each quintuplet contains an original HR\nground-truth (GT) patch and four enhanced GTs. Each of\nthe four enhanced GTs in each quintuplet is annotated by\nhuman annotators for being better than (‘positive’), similar\nto (‘similar’), or worse than (‘negative’) the original GT.\nWhile ‘positive’ labels are abundant, ‘negative’ labels are\nrare. Out of 20,193, only 1,270 quintuplets have at least\none ‘negative’. We analyze the seven metrics on this subset.\nWe evaluate the NR-IQA metrics based on the average\nSpearman rank correlation coefficient, and positive and neg-\native misalignment rates. Assuming higher rank is better,\nwe define positive (negative) misalignment rate as the frac-\ntion of quintuplets where at least one positive (negative) GT\nis ranked lower (higher) than at least one similar GT. We\nshow results in Table 4. Note that all metrics have poor\nnegative misalignment rate, leading to low Spearman cor-\nrelations. We believe that NR-IQA metrics fail to recog-\nnize negative GTs, since they may not necessarily have low\nquality (recall that all are enhanced GTs), or may have ar-\ntifacts that are unrecognizable without a reference image.\nNonetheless, MUSIQ has the lowest positive misalignment\nrate. Hence, we use MUSIQ in §4 for weighted sampling\nof the GTs and direct optimization (see also Supp. §7.1).\nSince TOPIQ has the 2nd lowest positive misalignment rate\nafter MUSIQ, we include it as an evaluation metric in §5.\n4. Methods\nWe next explore how to improve existing SR methods with\nthe results of our findings. Since our interest is in percep-\ntual quality and its use in multimodal SR, we build upon\nrecent work, Human Guided Ground-truth (HGGT) [12],\nwhich constructs a set of ground-truth images per input,\nwith varying quality, and uses human tests to rank their rela-\ntive quality. We begin by reviewing HGGT [12] (§4.1), and\nthen discuss two methods of applying neural IQA models\nto augment it: altering the choice of ground-truth set based\non an automated IQA weight (§4.2) and directly optimizing\nthe IQA model in a fine-tuning step (§4.3) .\n4.1. Background\nAs discussed in §3.2, the HGGT dataset includes (i) a set\nof images (“originals”), (ii) a set of four super-resolved ver-\nsions of each original (“enhanced GTs”), and (iii) human\nannotations for each enhanced GT (“positive”, “similar”, or\n“negative” meaning better than, indistinguishable from, or\nOriginal GT\nMUSIQ = 31.76\nPositive GT 1\nMUSIQ = 36.13\nPositive GT 2\nMUSIQ = 54.19\nFigure 2. Fine-Grained Comparison via NR-IQA. MUSIQ can\ndifferentiate the quality of two images, both marked as “positive”\nby human annotators.\nHigher MUSIQ indicates higher quality\n(zoom for details). Unlike HGGT models, which utilize a uni-\nform distribution over positives, our approach enables differently\nweighting them (§4.2).\nworse than the original). The set of positives provides mul-\ntimodal supervision, since each one is a disparate yet rea-\nsonable GT for learning. The HGGT work [12] then shows\nthat utilizing these synthetic GTs is useful for SR training,\nexploring several neural architectures and degradation set-\ntings. While HGGT explores several variants for utilizing\ntheir human labels, we focus on the simple but highly per-\nforming “positives-only” scenario, which performs equiva-\nlently or better than the variants utilizing negatives. In this\nscenario, at each training iteration, every input image su-\npervises the network with a GT chosen uniformly randomly\nfrom the positives. As is relatively standard in SR (e.g.,\n[37, 85]), HGGT models are trained with a combined loss:\n  \\labe l { eq:los s } \\ma t hc al {L} (\\ t heta |\\widehat {I},I) = \\lambda _{\\ell _1} || I - \\widehat {I} ||_1 + \\lambda _P d_\\mathrm {P}(\\widehat {I},I) + \\lambda _A D(\\widehat {I}), (1)\nwhere bI\n=\nfθ(ILQ) is the SR estimate of the low-\nresolution (or low-quality) input ILQ, via SR network fθ,\nI ∼UILQ[{I1, . . . , In}] is the randomly chosen GT (from\nthe set of positives corresponding to image ILQ), dP is a\nperceptual loss, and D is an adversarial discriminator.\nHowever, HGGT requires human labels, which are dif-\nficult to scale and often domain-dependent. In contrast, we\nexplore the opportunities afforded by neural no-reference\nimage quality assessment (NR-IQA) models, which not\nonly eschew human labels, but also confer additional ca-\npabilities – namely, the ability to provide more fine-grained\nnon-uniform sampling weights (§4.2) and to enable direct\noptimization via differentiability (§4.3).\n4.2. Reweighted Sampling\nWe explore a few straightforward alternatives to uniform\nsampling of the positives, via an IQA model. In particular,\nconsider the following simple formulation:\n  I &\\ s im \\mathcal {P}[ \nS_I\n \\mid  \\mathrm  { S o ftMax}_\n\\ta\nu ( Q(S_ I ) )  ], \\ \\ \\l abel {eq:gt_sample} Q(S_I) &= \\{Q(I_1),\\ldots ,Q(I_n)\\}, \\\\ S_I &= \\{I_1,\\ldots ,I_n\\}\\in \\{ A_I, P_I \\}\n(4)\n4\n"
    },
    {
      "page_num": 5,
      "text": "Method\nPaQ-2-PiQ [97]\nNIMA† [74]\nLIQE♡[101]\nARNIQA-TID⋆[1]\nQ-Align⋄[88]\nTOPIQ-NR [11]\nTrain Acc. (%)\n37.58\n44.21\n31.50\n42.05\n42.86\n25.71\nTest Acc. (%)\n33.10\n39.70\n30.70\n38.60\n41.06\n30.89\nTable 3. Phase II analysis on SBS180K (continued). Accuracy of top 6 NR-IQA metrics on consensus samples (from train plus test)\nwhere MUSIQ fails. †♡⋆⋄See Table 1.\nMethod\nPaQ-2-PiQ [97]\nNIMA† [74]\nMUSIQ◁[41]\nLIQE♡[101]\nARNIQA-TID⋆[1]\nQ-Align⋄[88]\nTOPIQ-NR [11]\nSRC ↑\n0.10\n0.17\n0.17\n0.03\n0.28\n0.20\n0.09\nPM ↓\n0.26\n0.28\n0.16\n0.58\n0.51\n0.39\n0.21\nNM ↓\n0.85\n0.79\n0.95\n0.64\n0.45\n0.63\n0.97\nTable 4. Analysis on HGGT subset. We evaluate Spearman’s rank correlation coefficient (SRC), positive misalignment (PM) rate, and\nnegative misalignment (NM) rate. †◁♡⋆⋄. See Table 1.\nwhere I is the sampled GT, τ > 0 is the softmax tem-\nperature, Q is the NR-IQA model (higher is better), P is\na discrete distribution over elements of SI (weighted by\nSoftMaxτ(Q(SI))), and SI is the set of possible GTs (ei-\nther choosing from all candidates, enhanced and original,\ndenoted AI, or just positive ones, PI). The HGGT algo-\nrithm simply uses SI = PI and τ →∞(i.e., the uniform\ndistribution); we explore different combinations, including\nτ →0 (the arg max choice). We illustrate the utility of NR-\nIQA-based sampling (as opposed to uniform) in Fig. 2, dis-\nplaying an example that humans rank equivalently as posi-\ntive, yet is more precisely distinguished by the neural asses-\nsor. We consider three NR-IQA-based sampling scenarios.\nSoftmax-All (SMA). Given the set of all GTs (i.e., SI =\nAI), we use an IQA-weighted distribution over GTs. This\nsetting uses no human data, and simply randomly chooses a\nGT at each iteration with a weight proportional to softmax-\nrescaled quality. We set τ to ensure a distribution between\nuniform and Kronecker delta (i.e., argmax).\nSoftmax-Positives (SMP). This approach actually builds\non the human data in HGGT, using the softmax-normalized\nIQA scores but only of the positives (i.e., SI = PI). This\nsetting is the most similar to the HGGT positives-only (or\nuniform distribution on positives), just with non-uniform\nweights (based on τ). We expect this to outperform SMA\nsampling, as it has access to direct human preferences.\nArgmax-online (AMO). The use of a neural IQA model\nconfers an additional capability that human data lacks:\nwe can dynamically determine sampling weights for new\npatches at training time. In previous scenarios, at training\ntime, we first pick one GT out of the four (Eq. 3), followed\nby random patch sampling from the selected GT. Instead,\nin the Argmax-online (AMO) scenario, we first sample a\nrandom patch from each GT, followed by selecting the best\npatch. To be specific, we sample one patch from the same\nrandom location from each GT, then run Q on each patch,\nand choose the best one (i.e., the arg max of Q values, so\nFigure 3. Structured Optimization Noise. Optimizing via an\nNR-IQA metric (MUSIQ [41]) generates structured artifacts (left),\nsimilar to an adversarial attack, while utilizing LoRA removes this\nnoise (right; see §4.3 and Supp. §10.2). Zoom in for details.\nτ →0). We eschew human data; hence, SI = AI. This\nenables a more fine-grained judgment (since quality is com-\nputed at the patch level), whereas human annotations cannot\nnecessarily be so easily extrapolated.\n4.3. Direct Optimization\nGiven a differentiable image quality estimator, Q, an obvi-\nous approach to improving our SR model is to simply in-\nclude Q in our objective function. To some extent, this has\nalready been explored for unconditional generative models\n(e.g., [16]). However, when Q is a neural network with\nmany parameters, this is unlikely to succeed; in essence,\ngradient descent will act like an “adversarial attack” on Q\n(e.g., [31]). It is well-known that such “attacks” are often\nable to dramatically alter the output of the objective net-\nwork (say, a classifier), while changing the optimized input\nin unintuitive or imperceptible ways (e.g., [45, 73]); in SR,\nthis could conceivably manifest as artifacts that fool Q into\nproviding a high score, since NR-IQA models are known\nto be susceptible to attacks (e.g., [33, 55]). In fact, without\nadditional regularization, this is precisely what happens: in\nFig. 3, we display the artifacts that appear when an SR net-\nwork is naively fine-tuned with Q (see also Supp. §10.2).\nThus, inspired by prior work [16], we utilize low-rank\nadaptation (LoRA) [36] to regularize the optimization. For-\nmally, we continue training as normal, but only on the\nLoRA weights, plus an additional NR-IQA loss term:\n  \\widet il d e {\\mat hc a l {L}}(\\phi |\\widehat {I},I) = \\mathcal {L}(\\phi |\\widehat {I},I) - \\lambda _Q Q(\\widehat {I}), \n(5)\n5\n"
    },
    {
      "page_num": 6,
      "text": "where ϕ are the LoRA parameters, bI = fθ,ϕ(ILQ), Q is an\nNR-IQA model (where higher is better), and L is defined\nin Eq. 1. Unless otherwise specified, we set λA = 0 when\nfine-tuning, since we are already including an image quality\nterm (for which the critic normally acts; though see §5.2).\nSee Supp. §8 for additional details.\n5. Experiments\nSetup. We consider the HGGT dataset under two degrada-\ntion settings for the super-resolution (SR) problem: (a) the\nstandard Real-ESRGAN scenario (two random degradation\nrounds) [85] and (b) a simplified setting with a single round,\nused in the HGGT paper. We use the ESRGAN (RRDB-\nbased) architecture [84, 85] for (a) and SwinIR [50] for (b).\nUnless otherwise stated, we use the same training settings\nas HGGT. Based on our analysis in §3, unless noted other-\nwise, we use MUSIQ [41] as our NR IQA model, Q, for\nboth weighted sampling and direct optimization.\nEvaluation. Following HGGT [12], we utilize their Test-\n100 held-out images for evaluation. We utilize two low-\nlevel distortion metrics, PSNR and SSIM [86]. We also\nuse three FR models, which act as mid-level visual met-\nrics: LPIPS [102], LPIPS-ST [28], and DISTS [20]. LPIPS-\nST [28] is a shift-tolerant form of LPIPS [102], improv-\ning robustness to small translations imperceptible to hu-\nmans but highly damaging to distortion measures. Finally,\nwe apply four NR-IQA metrics. Since we are interested\nin differentiating high-quality images, we choose MUSIQ\n[41] and TOPIQ [11] as they have the best positive mis-\nalignment scores (see Table 4).\nWe also include NIMA\n[74] and Q-Align [88] based on their complementarity with\nMUSIQ (see §3.1), which is directly optimized. Since Test-\n100 has multiple positive GTs per image, evaluations with\nreference-based metrics are averaged across all positives.\nBaselines.\nWe compare to the SOTA “positives-only”\nmodel for HGGT, which we denote UPos, as it uses a\nuniform distribution over positive samples.\nWe include\ntwo human-annotation-free baselines that do not incorpo-\nrate NR-IQA: “OrigsOnly”, which trains only with original\n(non-enhanced) GT, and “Rand”, which randomly chooses\na supervisory image from among all potential GTs. For our\nmethods, we can choose (a) an IQA-based sampling type\nand (b) IQA-based fine-tuning settings. The different sam-\npling methodologies (SMA, SMP, and AMO) are described\nin §4.2, while we denote the use of fine-tuning (see §4.3)\nwith the “FT” moniker.\nWe also consider two main FT\nvariations, FTIG and FTHP, described in §5.2. Our primary\nmethod combines the best settings for both IQA-based sam-\npling and optimization: the AMO+FT scheme.\n5.1. Empirical Results\nOur results on HGGT Test-100 are displayed in Table 5 and\nFig. 4. See Supp. §11 for RealSR results as well.\nMultimodal Training Boosts Performance. As in HGGT,\nwe can see the impact of enhanced GTs by comparing Orig-\nsOnly to UPos, which has greatly improved perceptual qual-\nity (LPIPS, DIST, and NR-IQA). We also consider the Rand\nbaseline, showing that even randomly sampling enhanced\nGTs is helpful for perceptual quality but not sufficient to\nreach UPos performance, the SoTA method from HGGT,\nenabled by human filtering of low-quality GTs.\nNeural IQA Sampling Outperforms Human Rankings.\nWe next investigate whether IQA-based sampling (SMA,\nSMP, and AMO) can outperform UPos, which relies on\nhuman rankings. On SwinIR, LPIPS and DISTS remain\nlargely unchanged, but LPIPS-ST and the NR metrics\nshow small improvements, especially for AMO. On Real-\nESRGAN, sampling with IQA improves both LPIPS and\nLPIPS-ST, but only AMO shows substantial improvements\non the NR metrics. Surprisingly, despite access to human la-\nbels, SMP is very similar to SMA, maintaining nearly iden-\ntical performance on low and mid level distortion measures,\nwith a marginal boost in NR image quality. In general, we\nfind AMO is consistently superior to both SMA and SMP,\nwhich suggests that selecting for quality (especially at the\nfine-grained level demanded in the online setting) is more\nimportant than simply having multiple GTs; AMO is also\nmeasurably better than UPos in terms of perceptual quality,\ndespite the lack of access to human annotation.\nIQA Fine-tuning Improves both Human and Neural\nSampling.\nWe examine the impact of fine-tuning (FT)\non NR-IQA. When used on top of human data, denoted\nUPos+FT, LPIPS and DISTS are the same (SwinIR) or\nslightly worse (RealESRGAN), but NR-IQA metrics uni-\nformly improve, as well as, interestingly, LPIPS-ST. This\ntrade-off of mid-level perceptual distortion for high-level\nquality is effectively an extension of the previously ob-\nserved balance, between pixel-level distortion versus per-\nceptual metrics. Indeed, we still see the latter compromise\nhere, in that FT always damages low-level distortion met-\nrics (PSNR and SSIM), despite the increases in perceptual\nquality. This is expected, since NR-based FT does not opti-\nmize to a particular SR solution, let alone the one(s) in the\ndataset. In addition, the results with LPIPS-ST indicate that\nit is more perceptual than LPIPS or DIST (i.e., more NR-\nIQA-like, though still full-reference). Overall, the perfor-\nmance boosts with FT suggest that useful information can\nbe extracted from neural NR-IQA models, even on top of a\nmodel with access to human annotations, providing a sim-\nple mechanism for improving image quality in SR models.\nUpper-bounding NR-IQA Evaluation Performance. In\naddition, we compute “gold standard” NR-IQA values for\nthe GT Test data, taking the best score among the original\nand enhanced images, providing a soft upper-bound on NR\nscores, if one were able to exactly reproduce the “best” GT\nvia the SR network. We find that altered sampling generally\n6\n"
    },
    {
      "page_num": 7,
      "text": "24.42\n56.62\n60.86\n65.14\n73.12\n47.59\n71.13\n74.13\n75.78\n77.31\nLR\nOriginal GT\nSwinIR-UPos\nSwinIR-AMO\nSwinIR-AMO+FT\n41.46\n52.51\n54.92\n66.12\n77.07\n33.52\n56.01\n63.01\n67.50\n71.15\nLR\nOriginal GT\nRESRGAN-UPos\nRESRGAN-AMO\nRESRGAN-AMO+FT\nFigure 4. Qualitative results with NR-IQA Guidance. Following the notation of Table 5, columns 3-5 are (top 2 rows) SwinIR-UPos,\nSwinIR-AMO, and SwinIR-AMO + FT, and (bottom 2 rows) Real-ESRGAN-UPos, Real-ESRGAN-AMO, and Real-ESRGAN-AMO +\nFT. We show MUSIQ scores in insets. Qualitatively, we see improved performance as we move from ‘UPos’ to ‘AMO’ to ‘AMO-FT’,\nshowcasing superiority of each method over the previous one. Zoom in for details. See also Supp. §10.1 for additional examples.\ndoes not reach these values, but FT is able to reach and even\nsurpass them in several scenarios.\nSuperior Perceptual Quality via Neural Sampling and\nFine-Tuning.\nFinally, we test the natural unification of\nIQA-based neural sampling with IQA-driven FT, denoted\nby the AMO+FT setting. We find that this combination sur-\npasses the SoTA HGGT approach (UPos), in terms of per-\nceptual quality, despite not using any human annotations.\nSpecifically, for SwinIR, AMO+FT incurs a small penalty\n(∼3-4%) on LPIPS and DISTS, but improves LPIPS-ST\nby ∼6% and is superior to every other method according\nto NR metrics.\nIn the RealESRGAN setting, compared\nto UPos, AMO+FT again obtains a large improvement on\nLPIPS-ST (∼12%), with neglible changes on the other mid-\nlevel metrics (∼2%). It also soundly surpasses UPos ac-\ncording to every NR metric. Interestingly, RealESRGAN-\nAMO+FT does not outperform RealESRGAN-UPos+FT;\nhowever, note that the latter has access to human annota-\ntions. We also present a user study in Supp. §12, finding a\npreference for AMO+FT over UPos. Altogether, these re-\nsults suggest (i) human annotated rankings on multiple GTs,\nat least when used naively for SR training, can be easily\nsurpassed via neural NR-IQA scores, and (ii) considerable\nimprovements to the perceptual quality of SR models can\nbe attained through automated means, by simply applying\nexisting NR-IQA models.\n7\n"
    },
    {
      "page_num": 8,
      "text": "Model\nFR Low-Lev. Dist.\nFR Mid-Lev. Dist.\nNR High-Lev. Perceptual Quality\nPSNR ↑\nSSIM ↑\nLPIPS ↓\nLPIPS-ST ↓\nDISTS ↓\nMUSIQ ↑\nNIMA ↑\nQ-Align ↑\nTOPIQ ↑\nGold Standard\n✗\n–\n–\n–\n–\n–\n69.64\n5.28\n3.78\n0.69\nSwinIR-OrigsOnly\n✓\n22.72\n0.652\n0.227\n0.174\n0.162\n59.47\n4.87\n3.17\n0.48\nSwinIR-Rand\n✓\n22.45\n0.650\n0.180\n0.139\n0.131\n65.27\n5.11\n3.52\n0.59\nSwinIR-UPos∗\n✗\n22.30\n0.647\n0.169\n0.129\n0.123\n66.39\n5.16\n3.56\n0.62\nSwinIR-SMA\n✓\n22.27\n0.646\n0.171\n0.129\n0.124\n66.73\n5.16\n3.60\n0.63\nSwinIR-SMP\n✗\n22.29\n0.647\n0.171\n0.130\n0.124\n66.83\n5.17\n3.62\n0.62\nSwinIR-AMO\n✓\n22.08\n0.641\n0.167\n0.124\n0.123\n68.08\n5.21\n3.67\n0.66\nSwinIR-UPos + FTHP\n✗\n22.17\n0.642\n0.166\n0.123\n0.122\n68.38\n5.23\n3.64\n0.65\nSwinIR-UPos + FTIG\n✗\n22.03\n0.635\n0.168\n0.122\n0.123\n69.37\n5.24\n3.69\n0.66\nSwinIR-UPos + FT\n✗\n22.01\n0.633\n0.169\n0.123\n0.124\n69.70\n5.26\n3.70\n0.67\nSwinIR-AMO + FT\n✓\n21.77\n0.624\n0.174\n0.121\n0.128\n70.81\n5.29\n3.75\n0.70\nRESRGAN-OrigsOnly\n✓\n22.10\n0.618\n0.283\n0.229\n0.185\n57.91\n4.84\n2.99\n0.46\nRESRGAN-Rand\n✓\n21.66\n0.611\n0.234\n0.190\n0.160\n64.82\n5.18\n3.40\n0.60\nRESRGAN-UPos∗\n✗\n21.54\n0.608\n0.233\n0.192\n0.158\n65.93\n5.25\n3.47\n0.63\nRESRGAN-SMA\n✓\n21.46\n0.606\n0.227\n0.182\n0.157\n65.87\n5.23\n3.46\n0.63\nRESRGAN-SMP\n✗\n21.44\n0.607\n0.226\n0.182\n0.156\n66.66\n5.24\n3.51\n0.64\nRESRGAN-AMO\n✓\n21.28\n0.602\n0.224\n0.178\n0.156\n67.86\n5.29\n3.56\n0.66\nRESRGAN-UPos + FTHP\n✗\n21.30\n0.595\n0.226\n0.175\n0.158\n70.28\n5.32\n3.65\n0.69\nRESRGAN-UPos + FTIG\n✗\n21.14\n0.586\n0.236\n0.182\n0.160\n72.01\n5.35\n3.70\n0.70\nRESRGAN-UPos + FT\n✗\n21.09\n0.580\n0.235\n0.179\n0.163\n72.69\n5.37\n3.69\n0.71\nRESRGAN-AMO + FT\n✓\n21.02\n0.581\n0.228\n0.169\n0.161\n71.67\n5.35\n3.68\n0.71\nTable 5. Evaluation on held-out HGGT Test-100. “FR Low-Lev Dist” refers to full-reference low-level distance metrics; “FR Mid-Lev\nDist” and “NR High-Lev. Perceptual Quality” refer to full-reference and no-reference perceptual metrics, respectively. Second column\n(\n) indicates that a method works with no human GT ranking data (✓), or requires such GT annotations (✗). “Gold Standard” shows\nthe average of best metric value per quintuplet of test GTs. “OrigsOnly” means no multimodal supervision (no enhanced GT). “Rand”\nsignifies random GT choice (from both enhanced and original), which requires no human annotation, while “UPos” denotes the “positives-\nonly” scenario (uniform sampling from human-ranked positives), the SoTA baseline method from HGGT (marked by ∗). “FT” refers to\nfine-tuning (direct optimization); “FTHP” denotes using a higher perceptual loss weight (λP ), and “FTIG” the inclusion of GAN loss.\n5.2. Ablations and Variations\nIQA Optimization Refines the Perception–Distortion\nTrade-off. As noted, our results show a trade-off between\nmid-level perceptual and NR-IQA metrics, reminiscent of\nthe classic perception–distortion curve [5, 6] (which we also\nobserve, via PSNR and SSIM). We can control this mid-\nversus-high-level perceptual tradeoff, by simply changing\nthe FT loss weights.\nFor instance, comparing UPos+FT\nto UPos+FTHP (which increases λP ), we see that mid-level\nmetrics all improve, while all NR metrics decline.\nDiscriminators as IQA. One can naturally interpret the dis-\ncriminator (or critic) of a GAN as a form of NR-IQA model\n– in fact, it is one specialized to the errors and artifacts of the\nSR function we are training. However, the FTIG scenario,\nwhich keeps the GAN loss during FT, does not greatly im-\npact results (compared to the FT setting); in fact, it largely\ninduces very slight declines. We also tried UPos+FT with-\nout an NR-IQA model, instead simply upweighting the\nGAN loss (treating the critic as an IQA model); however,\nthis results in uniformly worse NR scores, with little change\nto mid-level metrics (see Supp. §9).\nAlternative NR-IQA. While we selected MUSIQ based\non our analysis (§3), we also tested FT with an alternative\nIQA model, PaQ-2-PiQ, based on its high score in Table 2\n(see Supp. §9). We find that NIMA, Q-Align, TOPIQ, and\n(unsurprisingly) MUSIQ all decline, for both SwinIR and\nRealESRGAN. Nevertheless, it is plausible that a different\nIQA model (or combination thereof), particularly if fine-\ntuned for SR, would provide a superior learning signal.\n6. Conclusion\nAs an ill-posed inverse problem, SR struggles with the di-\nchotomy between perceptual quality and reference fidelity.\nPrior research utilized multiple GTs and human annotations\nto mitigate this trade-off. In contrast, herein, we focus on\nimproving perceptual quality via neural IQA, enabling us to\neschew human annotations. We first analyze existing NR-\nIQA methods, discerning a candidate for adoption in train-\ning, as well as complementary models for evaluation. Then,\nwe devised two ways to apply NR-IQA to SR training: (i)\nIQA-weighted multimodal GT sampling and (ii) regularized\noptimization of NR quality. When jointly utilized, our ap-\nproach outperforms the existing SoTA, which relies on hu-\nman data, in terms of NR metrics, without sacrificing mid-\nlevel reference-based quality scores. We hope it enables fu-\nture investigation into NR-IQA for SR, the connection be-\ntween generative modelling and IQA, and SR with domain\nshift, as IQA does not need paired GT.\n8\n"
    },
    {
      "page_num": 9,
      "text": "References\n[1] Lorenzo\nAgnolucci,\nLeonardo\nGalteri,\nMarco\nBertini, and Alberto Del Bimbo. ARNIQA: Learning\ndistortion manifold for image quality assessment. In\nWinter Conference on Applications of Computer Vi-\nsion (WACV), 2024. 3, 5\n[2] Namhyuk Ahn, Byungkon Kang, and Kyung-Ah\nSohn.\nFast,\naccurate,\nand lightweight super-\nresolution with cascading residual network. In Euro-\npean Conference on Computer Vision (ECCV), 2018.\n2\n[3] Yuval Bahat and Tomer Michaeli. Explorable super\nresolution. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2020. 2\n[4] Simon Baker and Takeo Kanade. Limits on super-\nresolution and how to break them. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence\n(PAMI), 2002. 1\n[5] Yochai Blau and Tomer Michaeli. The perception-\ndistortion tradeoff. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2018. 2, 8,\n16, 18\n[6] Yochai Blau and Tomer Michaeli. Rethinking lossy\ncompression: The rate-distortion-perception trade-\noff. In International Conference on Machine Learn-\ning (ICML), 2019. 2, 8\n[7] Yochai Blau, Roey Mechrez, Radu Timofte, Tomer\nMichaeli, and Lihi Zelnik-Manor. The 2018 PIRM\nchallenge on perceptual image super-resolution. In\nEuropean Conference on Computer Vision Work-\nshops (ECCVW), 2018. 3\n[8] Sebastian Bosse, Dominique Maniry, Klaus-Robert\nM¨uller, Thomas Wiegand, and Wojciech Samek.\nDeep neural networks for no-reference and full-\nreference image quality assessment. IEEE Transac-\ntions on Image Processing (TIP), 2017. 3\n[9] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng\nCao, and Lei Zhang. Toward real-world single im-\nage super-resolution: A new benchmark and a new\nmodel. In International Conference on Computer Vi-\nsion (ICCV), 2019. 16, 18\n[10] Chaofeng Chen and Jiadi Mo.\nIQA-PyTorch: Py-\ntorch toolbox for image quality assessment.\n[On-\nline]. Available:\nhttps : / / github . com /\nchaofengc/IQA-PyTorch, 2022. 3\n[11] Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning\nWu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi\nLin. TOPIQ: A top-down approach from semantics\nto distortions for image quality assessment.\nIEEE\nTransactions on Image Processing (TIP), 2024. 3, 5,\n6, 14\n[12] Du Chen, Jie Liang, Xindong Zhang, Ming Liu, Hui\nZeng, and Lei Zhang. Human guided ground-truth\ngeneration for realistic image super-resolution.\nIn\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 2, 3, 4, 6\n[13] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and\nJian Sun. Simple baselines for image restoration. In\nEuropean Conference on Computer Vision (ECCV),\n2022. 2\n[14] Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao,\nand Chao Dong.\nActivating more pixels in image\nsuper-resolution transformer. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR),\n2023. 2\n[15] Zheng Chen, Zongwei Wu, Eduard Zamfir, Kai\nZhang, Yulun Zhang, Radu Timofte, Xiaokang Yang,\nHongyuan Yu, Cheng Wan, Yuxin Hong, Zhijuan\nHuang, Yajun Zou, Yuan Huang, Jiamin Lin, Bing-\nnan Han, Xianyu Guan, Yongsheng Yu, Daoan\nZhang, Xuanwu Yin, Kunlong Zuo, Jinhua Hao, Kai\nZhao, Kun Yuan, Ming Sun, Chao Zhou, Hongyu An,\nXinfeng Zhang, Zhiyuan Song, Ziyue Dong, Qing\nZhao, Xiaogang Xu, Pengxu Wei, Zhi chao Dou, Gui\nling Wang, Chih-Chung Hsu, Chia-Ming Lee, Yi-\nShiuan Chou, Cansu Korkmaz, A. Murat Tekalp, Yu-\nbin Wei, Xiaole Yan, Binren Li, Haonan Chen, Siqi\nZhang, Sihan Chen, Amogh Joshi, Nikhil Akalwadi,\nSampada Malagi, Palani Yashaswini, Chaitra De-\nsai, Ramesh Ashok Tabib, Ujwala Patil, Uma Mude-\nnagudi, Anjali Sarvaiya, Pooja Choksy, Jagrit Joshi,\nShubh Kawa, Kishor Upla, Sushrut Patwardhan,\nRaghavendra Ramachandra, Sadat Hossain, Geongi\nPark, S. M. Nadim Uddin, Hao Xu, Yanhui Guo,\nAman Urumbekov, Xingzhuo Yan, Wei Hao, Ming-\nhan Fu, Isaac Orais, Samuel Smith, Ying Liu, Wang-\nwang Jia, Qisheng Xu, Kele Xu, Weijun Yuan, Zhan\nLi, Wenqin Kuang, Ruijin Guan, Ruting Deng, Zhao\nZhang, Bo Wang, Suiyi Zhao, Yan Luo, Yanyan Wei,\nAsif Hussain Khan, Christian Micheloni, and Niki\nMartinel. NTIRE 2024 challenge on image super-\nresolution (×4): Methods and results, 2024. 2\n[16] Kevin Clark, Paul Vicol, Kevin Swersky, and David J\nFleet. Directly fine-tuning diffusion models on dif-\nferentiable rewards.\nInternational Conference on\nLearning Representations (ICLR), 2024. 2, 3, 5\n[17] Mauricio Delbracio and Peyman Milanfar. Inversion\nby direct iteration: An alternative to denoising diffu-\nsion for image restoration. Transactions on Machine\nLearning Research (TMLR), 2023. 2\n[18] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Si-\nmoncelli. Image quality assessment: Unifying struc-\nture and texture similarity.\nIEEE Transactions on\nPattern Analysis and Machine Intelligence (PAMI),\n2020. 2\n[19] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P Si-\n9\n"
    },
    {
      "page_num": 10,
      "text": "moncelli. Comparison of full-reference image qual-\nity models for optimization of image processing sys-\ntems.\nInternational Journal of Computer Vision\n(IJCV), 2021. 2, 3\n[20] Keyan Ding, Kede Ma, Shiqi Wang, and Eero P. Si-\nmoncelli. Image quality assessment: Unifying struc-\nture and texture similarity.\nIEEE Transactions on\nPattern Analysis and Machine Intelligence (PAMI),\n2022. 6\n[21] Chao Dong, Chen Change Loy, Kaiming He, and Xi-\naoou Tang. Learning a deep convolutional network\nfor image super-resolution. In European Conference\non Computer Vision (ECCV), 2014. 1, 2\n[22] Chao Dong, Chen Change Loy, and Xiaoou Tang.\nAccelerating the super-resolution convolutional neu-\nral network. In European Conference on Computer\nVision (ECCV), 2016. 1, 2\n[23] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu,\nMoonkyung Ryu, Craig Boutilier, Pieter Abbeel,\nMohammad Ghavamzadeh, Kangwook Lee, and\nKimin Lee. DPOK: reinforcement learning for fine-\ntuning text-to-image diffusion models. In Neural In-\nformation Processing Systems (NeurIPS), 2023. 2\n[24] William T Freeman, Thouis R Jones, and Egon C\nPasztor.\nExample-based super-resolution.\nIEEE\nComputer graphics and Applications, 2002. 1\n[25] Stephanie Fu, Netanel Tamir, Shobhita Sundaram,\nLucy Chai, Richard Zhang, Tali Dekel, and Phillip\nIsola. DreamSim: Learning new dimensions of hu-\nman visual similarity using synthetic data. Neural\nInformation Processing Systems (NeurIPS), 2023. 19\n[26] Kevin Galim, Wonjun Kang, Yuchen Zeng, Hyung Il\nKoo, and Kangwook Lee. Parameter-efficient fine-\ntuning of state space models.\narXiv preprint\narXiv:2410.09016, 2024. 14\n[27] Sara Ghazanfari, Siddharth Garg, Prashanth Krish-\nnamurthy, Farshad Khorrami, and Alexandre Araujo.\nR-LPIPS: An adversarially robust perceptual similar-\nity metric. arXiv preprint arXiv:2307.15157, 2023.\n2\n[28] Abhijay Ghildyal and Feng Liu. Shift-tolerant per-\nceptual similarity metric. In European Conference\non Computer Vision (ECCV), 2022. 6\n[29] S Alireza Golestaneh, Saba Dadsetan, and Kris M\nKitani. No-reference image quality assessment via\ntransformers, relative ranking, and self-consistency.\nIn Winter Conference on Applications of Computer\nVision (WACV), 2022. 3\n[30] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio.\nGenerative adver-\nsarial nets. Neural Information Processing Systems\n(NeurIPS), 2014. 2\n[31] Ian J Goodfellow, Jonathon Shlens, and Christian\nSzegedy. Explaining and harnessing adversarial ex-\namples. International Conference on Learning Rep-\nresentations (ICLR), 2015. 5\n[32] Hang Guo, Jinmin Li, Tao Dai, Zhihao Ouyang,\nXudong Ren, and Shu-Tao Xia. MambaIR: A sim-\nple baseline for image restoration with state-space\nmodel. In European Conference on Computer Vision\n(ECCV), 2024. 2\n[33] Alexander Gushchin, Khaled Abud, Georgii By-\nchkov, Ekaterina Shumitskaya, Anna Chistyakova,\nSergey Lavrushkin, Bader Rasheed, Kirill Maly-\nshev, Dmitriy Vatolin, and Anastasia Antsiferova.\nGuardians of image quality: Benchmarking defenses\nagainst adversarial attacks on image quality metrics.\narXiv preprint arXiv:2408.01541, 2024. 5\n[34] Martin Heusel, Hubert Ramsauer, Thomas Un-\nterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGANs trained by a two time-scale update rule con-\nverge to a local Nash equilibrium. Neural Informa-\ntion Processing Systems (NeurIPS), 2017. 3\n[35] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar\nSaupe. KonIQ-10k: An ecologically valid database\nfor deep learning of blind image quality assessment.\nIEEE Transactions on Image Processing (TIP), 2020.\n3, 14\n[36] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of\nlarge language models. International Conference on\nLearning Representations (ICLR), 2022. 5, 14\n[37] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang,\nJilin Li, and Feiyue Huang.\nReal-world super-\nresolution via kernel estimation and noise injection.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition Workshops (CVPRW), 2020. 4\n[38] Younghyun Jo, Seoung Wug Oh, Peter Vajda, and\nSeon Joo Kim. Tackling the ill-posedness of super-\nresolution through adaptive target generation.\nIn\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 2\n[39] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Per-\nceptual losses for real-time style transfer and super-\nresolution. In European Conference on Computer Vi-\nsion (ECCV), 2016. 2\n[40] Bahjat Kawar, Michael Elad, Stefano Ermon, and Ji-\naming Song. Denoising diffusion restoration models.\nNeural Information Processing Systems (NeurIPS),\n2022. 2\n[41] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milan-\nfar, and Feng Yang. MUSIQ: Multi-scale image qual-\nity transformer. In International Conference on Com-\nputer Vision (ICCV), 2021. 2, 3, 5, 6, 14\n10\n"
    },
    {
      "page_num": 11,
      "text": "[42] Markus Kettunen, Erik H¨ark¨onen, and Jaakko Lehti-\nnen. E-LPIPS: robust perceptual image similarity via\nrandom transformation ensembles.\narXiv preprint\narXiv:1906.03973, 2019. 2\n[43] Valentin Khrulkov and Artem Babenko.\nNeural\nside-by-side: Predicting human preferences for no-\nreference super-resolution evaluation. In IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2021. 3, 14\n[44] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee.\nDeeply-recursive convolutional network for image\nsuper-resolution. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2016. 2\n[45] Alexey Kurakin, Ian Goodfellow, and Samy Ben-\ngio.\nAdversarial machine learning at scale.\nIn-\nternational Conference on Learning Representations\n(ICLR), 2017. 5\n[46] Yongchan Kwon, Eric Wu, Kevin Wu, and James\nZou. DataInf: Efficiently estimating data influence\nin LoRA-tuned LLMs and diffusion models.\nIn-\nternational Conference on Learning Representations\n(ICLR), 2024. 14\n[47] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and\nMing-Hsuan Yang.\nDeep Laplacian pyramid net-\nworks for fast and accurate super-resolution. In IEEE\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), 2017. 2\n[48] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose\nCaballero, Andrew Cunningham, Alejandro Acosta,\nAndrew Aitken, Alykhan Tejani, Johannes Totz, Ze-\nhan Wang, et al. Photo-realistic single image super-\nresolution using a generative adversarial network. In\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017. 2\n[49] Xiaoming Li, Wangmeng Zuo, and Chen Change\nLoy.\nLearning generative structure prior for blind\ntext image super-resolution. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR),\n2023. 19\n[50] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai\nZhang, Luc Van Gool, and Radu Timofte. SwinIR:\nImage restoration using swin transformer. In Interna-\ntional Conference on Computer Vision (ICCV), 2021.\n2, 6, 14\n[51] Jie Liang, Hui Zeng, and Lei Zhang. Details or ar-\ntifacts: A locally discriminative learning approach\nto realistic image super-resolution.\nIn IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 2\n[52] Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Ar-\nseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi\nPont-Tuset, Sarah Young, Feng Yang, et al. Rich hu-\nman feedback for text-to-image generation. In IEEE\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), 2024. 3\n[53] Liting Lin, Heng Fan, Zhipeng Zhang, Yaowei\nWang, Yong Xu, and Haibin Ling. Tracking meets\nLoRA: Faster training, larger model, stronger perfor-\nmance. In European Conference on Computer Vision\n(ECCV), 2024. 14\n[54] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu,\nLingxi Xie, Yaowei Wang, Qixiang Ye, Jianbin Jiao,\nand Yunfan Liu.\nVMamba:\nVisual state space\nmodel.\nIn Neural Information Processing Systems\n(NeurIPS), 2024. 2\n[55] Yujia Liu, Chenxi Yang, Dingquan Li, Jianhao Ding,\nand Tingting Jiang. Defense against adversarial at-\ntacks on no-reference image quality models with gra-\ndient norm regularization. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR),\n2024. 5\n[56] Andreas Lugmayr, Martin Danelljan, Luc Van Gool,\nand Radu Timofte.\nSRFlow: Learning the super-\nresolution space with normalizing flow. In European\nConference on Computer Vision (ECCV), 2020. 2\n[57] Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and\nMing-Hsuan Yang. Learning a no-reference quality\nmetric for single-image super-resolution. Computer\nVision and Image Understanding (CVIU), 2017. 3\n[58] Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen,\nJiwen Lu, and Jie Zhou. Structure-preserving super\nresolution with gradient guidance. In IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), 2020. 2\n[59] Anish\nMittal,\nAnush\nKrishna\nMoorthy,\nand\nAlan Conrad Bovik.\nNo-reference image quality\nassessment in the spatial domain. IEEE Transactions\non Image Processing (TIP), 2012. 3, 14\n[60] Jim Nilsson and Tomas Akenine-M¨oller.\nUnder-\nstanding SSIM.\narXiv preprint arXiv:2006.13846,\n2020. 18\n[61] Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin,\nChen Change Loy, and Ping Luo. Exploiting deep\ngenerative prior for versatile image restoration and\nmanipulation. IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence (PAMI), 2021. 2\n[62] JoonKyu Park, Sanghyun Son, and Kyoung Mu Lee.\nContent-aware local GAN for photo-realistic super-\nresolution. In International Conference on Computer\nVision (ICCV), 2023. 2\n[63] Seung Ho Park, Young Su Moon, and Nam Ik Cho.\nPerception-oriented single image super-resolution\nusing optimal objective estimation.\nIn IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 2\n11\n"
    },
    {
      "page_num": 12,
      "text": "[64] Jonathan W Peirce.\nUnderstanding mid-level rep-\nresentations in visual processing. Journal of Vision\n(JOV), 2015. 19\n[65] Zygmunt Pizlo.\nPerception viewed as an inverse\nproblem. Vision research, 41(24):3145–3161, 2001.\n1\n[66] Nikolay Ponomarenko, Oleg Ieremeiev, Vladimir\nLukin, Karen Egiazarian, Lina Jin, Jaakko Astola,\nBenoit Vozel, Kacem Chehdi, Marco Carli, Feder-\nica Battisti, et al. Color image database TID2013:\nPeculiarities and preliminary results.\nIn European\nworkshop on visual information processing (EUVIP).\nIEEE, 2013. 3\n[67] Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak,\nand Katerina Fragkiadaki.\nAligning text-to-image\ndiffusion models with reward backpropagation.\narXiv preprint arXiv:2310.03739, 2023. 2\n[68] Michele A Saad, Alan C Bovik, and Christophe\nCharrier. A DCT statistics-based blind image quality\nindex. IEEE Signal Processing Letters, 2010. 2, 3\n[69] Chitwan Saharia,\nJonathan Ho,\nWilliam Chan,\nTim Salimans, David J Fleet, and Mohammad\nNorouzi. Image super-resolution via iterative refine-\nment. IEEE Transactions on Pattern Analysis and\nMachine Intelligence (PAMI), 2022. 2\n[70] Divya Saxena and Jiannong Cao. Generative adver-\nsarial networks (GANs) challenges, solutions, and\nfuture directions. ACM Computing Surveys (CSUR),\n2021. 2\n[71] Wenzhe Shi, Jose Caballero, Ferenc Husz´ar, Jo-\nhannes Totz, Andrew P Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang.\nReal-time single im-\nage and video super-resolution using an efficient sub-\npixel convolutional neural network. In IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2016. 2\n[72] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin\nGe, Jinqiu Sun, and Yanning Zhang. Blindly assess\nimage quality in the wild guided by a self-adaptive\nhyper network. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2020. 3\n[73] Christian\nSzegedy,\nWojciech\nZaremba,\nIlya\nSutskever, Joan Bruna, Dumitru Erhan, Ian Goodfel-\nlow, and Rob Fergus. Intriguing properties of neural\nnetworks. In International Conference on Learning\nRepresentations (ICLR), 2014. 5\n[74] Hossein Talebi and Peyman Milanfar. NIMA: Neu-\nral image assessment. IEEE Transactions on Image\nProcessing (TIP), 2018. 2, 3, 5, 6\n[75] Huixuan Tang, Neel Joshi, and Ashish Kapoor.\nLearning a blind measure of perceptual image qual-\nity. In IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2011. 2, 3\n[76] Radu Timofte, Vincent De Smet, and Luc Van Gool.\nAnchored neighborhood regression for fast example-\nbased super-resolution. In International Conference\non Computer Vision (ICCV), 2013. 2\n[77] Radu Timofte, Vincent De Smet, and Luc Van Gool.\nA+: Adjusted anchored neighborhood regression for\nfast super-resolution.\nIn Proceedings of the Asian\nConference on Computer Vision (ACCV), 2015. 2\n[78] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and\nBernhard Schoelkopf.\nWasserstein auto-encoders.\narXiv preprint arXiv:1711.01558v3, 2018. 19\n[79] Rao Muhammad Umer and Christian Micheloni.\nDeep cyclic generative adversarial residual convo-\nlutional networks for real image super-resolution.\nIn European Conference on Computer Vision Work-\nshops (ECCVW), 2020. 2\n[80] International Telecommunication Union. Methodol-\nogy for the subjective assessment of the quality of\ntelevision pictures. Recommendation ITU-R BT.500-\n13, 2012. Geneva, Switzerland. 18\n[81] Narasimhan\nVenkatanath,\nD\nPraneeth,\nMaruthi Chandrasekhar Bh, Sumohana S Chan-\nnappayya, and Swarup S Medasani.\nBlind image\nquality evaluation using perception based fea-\ntures. In 2015 Twenty first national conference on\ncommunications (NCC). IEEE, 2015. 3\n[82] Jianyi Wang, Kelvin CK Chan, and Chen Change\nLoy. Exploring CLIP for assessing the look and feel\nof images. In Proceedings of the National Confer-\nence on Artificial Intelligence (AAAI), 2023. 3\n[83] Jianyi Wang, Zongsheng Yue, Shangchen Zhou,\nKelvin CK Chan, and Chen Change Loy. Exploiting\ndiffusion prior for real-world image super-resolution.\nInternational Journal of Computer Vision (IJCV),\n2024. 2\n[84] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao\nLiu, Chao Dong, Yu Qiao, and Chen Change Loy.\nESRGAN: Enhanced super-resolution generative ad-\nversarial networks. In European Conference on Com-\nputer Vision Workshops (ECCVW), 2018. 2, 6, 14\n[85] Xintao Wang, Liangbin Xie, Chao Dong, and Ying\nShan.\nReal-ESRGAN: Training real-world blind\nsuper-resolution with pure synthetic data. In Interna-\ntional Conference on Computer Vision (ICCV), 2021.\n4, 6, 14\n[86] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and\nEero P Simoncelli. Image quality assessment: from\nerror visibility to structural similarity. IEEE Trans-\nactions on Image Processing (TIP), 2004. 2, 6\n[87] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng\nChen, Jingwen Hou, Annan Wang, Wenxiu Sun,\nQiong Yan, and Weisi Lin. Exploring video quality\nassessment on user generated contents from aesthetic\n12\n"
    },
    {
      "page_num": 13,
      "text": "and technical perspectives. In International Confer-\nence on Computer Vision (ICCV), 2023. 18\n[88] Haoning Wu,\nZicheng Zhang,\nWeixia Zhang,\nChaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao,\nAnnan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan,\nXiongkuo Min, Guangtao Zhai, and Weisi Lin. Q-\nAlign: Teaching LMMs for visual scoring via dis-\ncrete text-defined levels. In International Conference\non Machine Learning (ICML), 2024. 3, 5, 6, 14\n[89] Rongyuan Wu, Lingchen Sun, Zhiyuan Ma, and Lei\nZhang. One-step effective diffusion network for real-\nworld image super-resolution.\nNeural Information\nProcessing Systems (NeurIPS), 2024. 2, 18\n[90] Rongyuan\nWu,\nTao\nYang,\nLingchen\nSun,\nZhengqiang Zhang,\nShuai Li,\nand Lei Zhang.\nSeeSR: Towards semantics-aware real-world image\nsuper-resolution. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2024. 18\n[91] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang,\nXinglong Wu, Yapeng Tian, Wenming Yang, and Luc\nVan Gool. DiffIR: Efficient diffusion model for im-\nage restoration. In International Conference on Com-\nputer Vision (ICCV), 2023. 2\n[92] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong,\nQinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.\nImageReward: Learning and evaluating human pref-\nerences for text-to-image generation. Neural Infor-\nmation Processing Systems (NeurIPS), 2024. 2, 3\n[93] Jianchao Yang, Zhaowen Wang, Zhe Lin, Scott Co-\nhen, and Thomas Huang. Coupled dictionary train-\ning for image super-resolution. IEEE Transactions\non Image Processing (TIP), 2012. 2\n[94] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao,\nYuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu\nYang. MANIQA: Multi-dimension attention network\nfor no-reference image quality assessment. In IEEE\nConference on Computer Vision and Pattern Recog-\nnition (CVPR), 2022. 3\n[95] Tao Yang, Rongyuan Wu, Peiran Ren, Xuansong\nXie, and Lei Zhang.\nPixel-aware stable diffusion\nfor realistic image super-resolution and personalized\nstylization. In European Conference on Computer\nVision (ECCV), 2024. 2, 18\n[96] Jie-En Yao, Li-Yuan Tsao, Yi-Chen Lo, Roy Tseng,\nChia-Che Chang, and Chun-Yi Lee. Local implicit\nnormalizing flow for arbitrary-scale image super-\nresolution. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2\n[97] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv\nMahajan, Deepti Ghadiyaram, and Alan Bovik. From\npatches to pictures (PaQ-2-PiQ): Mapping the per-\nceptual space of picture quality.\nIn IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), 2020. 3, 5, 14\n[98] Lin Zhang, Lei Zhang, and Alan C Bovik. A feature-\nenriched completely blind image quality evaluator.\nIEEE Transactions on Image Processing (TIP), 2015.\n3, 14\n[99] Richard Zhang, Phillip Isola, Alexei A Efros, Eli\nShechtman, and Oliver Wang. The unreasonable ef-\nfectiveness of deep features as a perceptual metric.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018. 2\n[100] Weixia Zhang, Kede Ma, Jia Yan, Dexiang Deng, and\nZhou Wang. Blind image quality assessment using\na deep bilinear convolutional neural network. IEEE\nTransactions on Circuits and Systems for Video Tech-\nnology, 2020. 3\n[101] Weixia Zhang, Guangtao Zhai, Ying Wei, Xiaokang\nYang, and Kede Ma.\nBlind image quality assess-\nment via vision-language correspondence: A mul-\ntitask learning perspective. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR),\n2023. 3, 5\n[102] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bi-\nneng Zhong, and Yun Fu. Image super-resolution us-\ning very deep residual channel attention networks. In\nEuropean Conference on Computer Vision (ECCV),\n2018. 2, 6\n[103] Yuzhe Zhang, Jiawei Zhang, Hao Li, Zhouxia\nWang, Luwei Hou, Dongqing Zou, and Liheng Bian.\nDiffusion-based blind text image super-resolution. In\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2024. 19\n[104] Heliang Zheng, Huan Yang, Jianlong Fu, Zheng-Jun\nZha, and Jiebo Luo. Learning conditional knowledge\ndistillation for degraded-reference image quality as-\nsessment. In International Conference on Computer\nVision (ICCV), 2021. 3\n13\n"
    },
    {
      "page_num": 14,
      "text": "Augmenting Perceptual Super-Resolution via Image Quality Predictors\nSupplementary Material\n7. Complete Analysis of NR-IQA metrics\nIn §3.1 of the main paper, we present accuracy of only top\n7 NR-IQA metrics on the subset of SBS180K [43] train set.\nHere, in Table 6, we present accuracy of 20 NR-IQA met-\nrics and their variants (42 in total) on the same subset. We\nmake two main observations. First, unsurprisingly, recent\nNR-IQA metrics (e.g. PaQ-2-PiQ [97], MUSIQ [41], Q-\nAlign [88]) are more aligned with human preferences than\nthe classical ones (e.g. NIQE [98] and BRISQUE [59]),\ncalling for wider adaptation of more recent metrics in eval-\nuating SR models. Second, the IQA dataset on which the\nmetric is trained affects its accuracy in determining human\npreference for SR. For instance, TOPIQ [11] trained using\nKonIQ [35] dataset is more aligned with human judgement\n(73.06%) than the one trained using FLIVE [97] dataset\n(58.19%). Results indicate an opportunity to create a no-\nreference IQA dataset exclusively for training NR metrics\nfor SR.\n7.1. Remark on NR-IQA Choices\nAs discussed in §3, our choice of MUSIQ for weighted\nsampling and fine-tuning comes from several considera-\ntions. First, on SBS-180K, MUSIQ is highly performant\n(see Phase II analysis of §3.1). Second, on HGGT (§3.2),\nMUSIQ has the best positive misalignment, meaning it is\nthe least likely to misrank a positive. It is also relatively\nefficient for both inference and back-propagation.\nIn our application, distinguishing between the quality\nof positives would seem to be more significant, since we\nare ideally training the SR model in a manner that focuses\non the highest quality images (i.e., incorrect ordering of\nthe lower-ranked images will not affect our method; hence,\nfine-grained differentiation between the high-ranked images\nis more important). Nevertheless, it is true that MUSIQ\n(like all NR-IQA models evaluated here) does not perform\nwell on negative misalignment. However, we do not expect\nthis to have a large impact on training, due to the rarity of\nnegatives. Specifically, in HGGT-train, only ∼6% of tuples\ncontain negatives and, among those, MUSIQ ranks a nega-\ntive the highest in ∼34% of cases. Thus, our AMO model\nwill be exposed to a negative in only ∼2% of examples.\nHence, merely for numerical magnitude, discernment for\npositives is likely to be more impactful than for negatives.\nOf course, this reasoning is somewhat specific to the HGGT\nsetup.\nFurther, in terms of evaluation, note that we choose\nNIMA and Q-Align specifically because they perform best\non the SBS-180K samples on which MUSIQ fails. Ideally,\nthis complementarity would help ensure that errors induced\nby shortcomings of MUSIQ could potentially be detected\nby the other NR-IQA metrics. Nevertheless, as seen in Ta-\nble 7, our experiments with PaQ-2-PiQ (which was among\nthe best models according to Table 2) show MUSIQ outper-\nforms it.\nRegardless, our method does not specifically require the\nuse of MUSIQ. Indeed, we believe further advancements\nin NR-IQA models (e.g., approaches specific to SR image\nquality, adversarially robust models) will be applicable to\nour method as well.\n8. Methodological Details\n8.1. Sampling Details\nThe altered sampling (§4.2) is trained identically to the stan-\ndard HGGT version, just replacing the uniform nature of the\nGT sampling. The only additional parameter is the temper-\nature, τ, which we set to 10 for both SMA and SMP.\n8.2. Hardware and Timing\nSimilar to HGGT, we train on four A100 GPUs for 300K\niterations. This takes ∼23 and ∼32 hours for SwinIR and\nRealESRGAN, respectively, with an additional ∼3.5 hours\nfor fine-tuning.\n8.3. Fine-Tuning Details\nUnless otherwise noted, we use the same training parame-\nters as HGGT. We fine-tune for only 20,000 steps and set\nλQ = 0.05 as the FT MUSIQ weight. For SwinIR and\nRealESRGAN, respectively, we change the learning rate to\n5 × 10−6 (halved at 5K steps) and 5 × 10−5. Recall that,\nby default, the adversarial loss is not used (i.e., λA = 0)\nduring FT (but see §9). Architecturally, LoRA weights are\ninserted slightly differently: on SwinIR [50], only the mul-\ntilayer perceptions are altered (rank 48), while on the con-\nvolutional RealESRGAN [84, 85], only the layers in the\nResidual-in-Residual Dense Blocks (RRDBs) are altered\n(rank 24). This follows other works, including the origi-\nnal LoRA paper [36], which only apply LoRA-based fine-\ntuning to a subset of layers (e.g., see [26, 46, 53]). Recall\nthat LoRA cannot increase the capacity (i.e., expressive ca-\npability) of the networks (as the new weights can simply be\nmerged into the old ones at inference time, which also pre-\nvents any run-time penalty to inference), so comparisons to\nnon-FT models are fair. Fine-tuning is run for 20K steps,\nas opposed to the 300K in stage two training. Brief explo-\nration of hyper-parameters (beyond those considered in §5)\n14\n"
    },
    {
      "page_num": 15,
      "text": "Method\nAcc (%)\nMethod\nAcc (%)\nMethod\nAcc (%)\nMethod\nAcc (%)\nMethod\nAcc (%)\npaq2piq\n76.41\narniqa-kadid\n71.48\ntres\n69.98\narniqa-clive\n66.81\nbrisque matlab\n61.00\nnima\n74.91\narniqa-flive\n71.30\nclipiqa+ vitL14 512\n69.98\narniqa-spaq\n66.73\nwadiqam nr\n60.30\nmusiq\n74.47\ntopiq nr-spaq\n71.30\nmusiq-paq2piq\n69.98\narniqa\n66.46\ntopiq nr-flive\n58.19\nliqe\n74.03\narniqa-csiq\n71.21\nmaniqa-pipal\n69.63\nmusiq-ava\n66.37\nilniqe\n57.92\narniqa-tid\n74.03\nmusiq-spaq\n70.86\nclipiqa+ rn50 512\n69.10\nnrqm\n65.05\nniqe\n56.43\nqalign\n73.77\nnima-vgg16-ava\n70.77\ndbcnn\n68.49\ncnniqa\n63.73\nbrisque\n55.11\ntopiq nr\n73.06\nmaniqa\n70.51\nclipiqa\n68.40\ntres-flive\n63.29\nniqe matlab\n51.94\nhyperiqa\n72.27\nclipiqa+\n70.25\narniqa-live\n68.05\npi\n62.41\npiqe\n46.21\nliqe mix\n71.48\nmaniqa-kadid\n70.16\nTable 6. Phase I analysis on SBS180K dataset. Accuracy of 20 NR-IQA metrics and their variants on the subset (1212 image pairs)\nof train set of SBS180K dataset. We denote a metric by its ‘Model Name’ as defined in IQA-PyTorch toolbox (https://iqa-\npytorch.readthedocs.io/en/latest/ModelCard.html). We use the default configuration for all metrics and their variants.\nModel\nNR\nλA\nFR Low-Lev. Dist.\nFR Mid-Lev. Dist.\nNR High-Lev. Perceptual Quality\nPSNR ↑\nSSIM ↑\nLPIPS ↓LPIPS-ST ↓DISTS ↓\nMUSIQ ↑NIMA ↑\nQ-Align ↑\nTOPIQ ↑\nGold Standard\n–\n–\n✗\n–\n–\n–\n–\n–\n69.64\n5.28\n3.78\n0.69\nSwinIR-UPos∗\n–\n–\n✗\n22.30\n0.647\n0.169\n0.129\n0.123\n66.39\n5.16\n3.56\n0.62\nSwinIR-UPos + FTHP\nM\n0\n✗\n22.17\n0.642\n0.166\n0.123\n0.122\n68.38\n5.23\n3.64\n0.65\nSwinIR-UPos + FTIG\nM\n0.1\n✗\n22.03\n0.635\n0.168\n0.122\n0.123\n69.37\n5.24\n3.69\n0.66\nSwinIR-UPos + FTNNR,IG×2\n–\n0.2\n✗\n22.25\n0.646\n0.171\n0.130\n0.124\n66.61\n5.16\n3.56\n0.61\nSwinIR-UPos + FTNNR,IG×5\n–\n0.5\n✗\n22.20\n0.644\n0.174\n0.134\n0.125\n66.61\n5.16\n3.56\n0.61\nSwinIR-UPos + FTPaQ2PiQ\nP\n0\n✗\n22.29\n0.649\n0.166\n0.120\n0.121\n67.29\n5.18\n3.58\n0.62\nSwinIR-UPos + FT\nM\n0\n✗\n22.01\n0.633\n0.169\n0.123\n0.124\n69.70\n5.26\n3.70\n0.67\nSwinIR-AMO + FT\nM\n0\n✓\n21.77\n0.624\n0.174\n0.121\n0.128\n70.81\n5.29\n3.75\n0.70\nRESRGAN-UPos∗\n–\n–\n✗\n21.54\n0.608\n0.233\n0.192\n0.158\n65.93\n5.25\n3.47\n0.63\nRESRGAN-UPos + FTHP\nM\n0\n✗\n21.30\n0.595\n0.226\n0.175\n0.158\n70.28\n5.32\n3.65\n0.69\nRESRGAN-UPos + FTIG\nM\n0.1\n✗\n21.14\n0.586\n0.236\n0.182\n0.160\n72.01\n5.35\n3.70\n0.70\nRESRGAN-UPos + FTNNR,IG×2\n–\n0.2\n✗\n21.35\n0.600\n0.234\n0.191\n0.157\n65.94\n5.22\n3.45\n0.63\nRESRGAN-UPos + FTNNR,IG×5\n–\n0.5\n✗\n21.25\n0.598\n0.237\n0.195\n0.158\n65.78\n5.22\n3.46\n0.63\nRESRGAN-UPos + FTPaQ2PiQ\nP\n0\n✗\n21.46\n0.605\n0.228\n0.182\n0.157\n67.26\n5.22\n3.51\n0.64\nRESRGAN-UPos + FT\nM\n0\n✗\n21.09\n0.580\n0.235\n0.179\n0.163\n72.69\n5.37\n3.69\n0.71\nRESRGAN-AMO + FT\nM\n0\n✓\n21.02\n0.581\n0.228\n0.169\n0.161\n71.67\n5.35\n3.68\n0.71\nTable 7. Additional evaluation on held-out HGGT Test-100. As in Table 5 in the main paper, “FR Low-Lev Dist” refers to full-reference\nlow-level distance metrics; “FR Mid-Lev Dist” and “NR High-Lev. Perceptual Quality” refer to full-reference and no-reference perceptual\nmetrics, respectively. Second column (\n) indicates that a method works with no human GT ranking data (✓), or requires such GT\nannotations (✗). “Gold Standard” shows the average of best metric value per quintuplet of test GTs. “UPos” denotes the “positives-only”\nscenario (uniform sampling from human-ranked positives), the SoTA baseline method from HGGT (marked by ∗). “FT” refers to fine-\ntuning (direct optimization): “FTIG” includes the adversarial loss during FT, “FTNNR,IG×2” and “FTNNR,IG×5” have no NR term during FT,\nbut increase the GAN loss (two and five times, respectively), and finally “FTPaQ2PiQ” replaces MUSIQ with PaQ-2-PiQ. The NR column\ndenotes which NR-IQA model is used (M: MUSIQ, P: PaQ-2-PiQ, –: None), while λA is the adversarial loss weight (the standard HGGT\ndefault for training is 0.1). We also show our best method: AMO+FT, which combines IQA-based sampling with our standard FT settings,\nfor comparison. Note that AMO+FT is the only method here that does not use human annotations. We remark also that the NR-IQA\nmodels have the following ranges: MUSIQ (0-100), NIMA (0-10), Q-Align (1,5), and TOPIQ (0-1).\nyielded minimal changes, likely due to rapid convergence\nof the low-rank (i.e., low capacity) weights ϕ.\n9. Detailed Results on Ablations and Variations\nIn this section, we consider additional FT variations: (i) us-\ning a GAN discriminator instead of an NR-IQA model (us-\ning two different loss weights) and (ii) replacing Q (set as\nMUSIQ) with a different NR-IQA model (PaQ-2-PiQ). The\npoint of (i) is to check whether the GAN critic, which is ef-\nfectively an NR-IQA model that has been specialized to the\nSR model in question, can be used for fine-tuning, instead\nof a separate NR-IQA model. For (ii), we wish to check if\nour choice of optimized NR metric, MUSIQ, is reasonable.\nOur results on these variations are in Table 7. Since FT\noptimizes MUSIQ, we focus on the other NR metrics, espe-\ncially Q-Align and NIMA (since they perform the best on\nexamples where MUSIQ fails; see §3.1). First, we find that\nincluding the GAN loss in the standard scenario has a slight\nnegative effect on the NR metrics; however, removing the\nNR metric term and strengthening the adversarial term (i.e.,\n“FTNNR,IG×2” and “FTNNR,IG×5”) has a significantly more\nnegative impact on the NR evaluations. This suggests that\n15\n"
    },
    {
      "page_num": 16,
      "text": "Figure 5. Structured Noise due to naive NR-IQA optimization. The left three insets show an image and two close-ups that was fine-tuned\nwithout LoRA, whereas the right three show the effect of using LoRA. Note the patterns that form in the sky and the strangely coloured\npixels that appear around certain edges (e.g., the blue/red grid in the second inset) when LoRA is not used.\nthe critic network cannot replace the NR-IQA model, even\nthough it is intuitively similar to one (in that it evaluates\nthe image quality of a single input, which can be used as a\nlearning signal). We conjecture this is because the critic is\ntrained to detect the idiosyncrasies of its associated gener-\nator (at a specific point in time), rather than match human\nquality estimates; hence, optimizing it more aggressively\nmay reduce those specific issues that the critic has detected,\nbut not necessarily increase general quality.\nSecond, we tried to replace MUSIQ with PaQ-to-PiQ.\nWe find that this tends to improve low and mid level distor-\ntion (though the relation is less clear for RealESRGAN, es-\npecially with LPIPS-ST), but worsens NIMA and Q-Align.\nWe therefore choose to stay with MUSIQ for our main re-\nsults. In general, we do not wish to claim that MUSIQ is an\noptimal starting point for FT; however, it does suggest our\nanalysis is a useful approach to initially identifying a good\nNR-IQA network. Nevertheless, we suspect that using an\nalternative NR-IQA model (with sufficient hyper-parameter\nexploration), fine-tuning a new model, combining multiple\nmodels, or training a model specific to SR could all be po-\ntentially useful future approaches to improving results.\n10. Additional Qualitative Examples\n10.1. Additional Comparative Samples\nAdditional comparisons are shown in Fig. 6 (as in Fig. 4).\nOur method (AMO or AMO+FT) is universally sharper\nand more detailed than UPos (e.g., see the hair in row\nthree).\nFurther, it can occasionally remove some of the\nnoise present in the UPos scenario (see the tongue of the red\npanda). Importantly, our approach may not generate details\nthat are identical to the GT, but it does construct sharp image\ncontent without jarring unrealistic artifacts (e.g., see rows\none and four; the plants, rocks, and bricks have slightly dif-\nferent details, but they are plausible and of similar aesthetic\nquality nonetheless).\n10.2. Additional Naive Optimization Visualizations\nIn Fig. 5, as in Fig. 3, we show the subtle “grid-like” arti-\nfacts that appear when naive NR-IQA optimization is per-\nformed. In particular, we see spatial patterns form in ho-\nmogeneous areas (e.g., stripes in the sky or on the tan\ncoloured island), while other areas exhibit highly unnatu-\nral colours (e.g., the alternating blue-red pixels on the dark\nrock). These small, pixel-scale artifacts are akin to an ad-\nversarial attack on MUSIQ; hence, much of this structured\nnoise is alleviated by applying LoRA (right insets). Other\nmethods of handling such artifacts, such as an adversarially\nrobust NR-IQA model, may also be effective, but we leave\nthis to future work.\n11. Additional Results on RealSR\nWe provide results on the RealSRv3 [9] dataset in Table 8.\nSimilar to the HGGT test dataset, we find that our method is\nsuperior in terms of every NR-IQA metric, at the expense of\nthe exact pixel-level details measured by PSNR and SSIM\n(following the perception-distortion tradeoff [5]). However,\naccording to mid-level FR metrics, our method also per-\nforms well, obtaining the best scores on LPIPS-ST, and\neven on LPIPS and DISTS for SwinIR. This suggests our\nmethod can improve image quality, while maintaining the\nmost salient perceptual details (e.g., mid-level textures) of\nthe underlying GT.\n12. Comparative Evaluation via User Study\nSimilar to the HGGT user study, we invite 12 volunteers\nto evaluate their preference between SwinIR-AMO+FT and\nSwinIR-UPos, using the HGGT Test-100 dataset.\nEach\nvolunteer evaluates 25 image pairs (25% of the dataset),\nwith each image in Test-100 being seen an equal number\nof times (namely, three). For each pair (SwinIR-AMO+FT\nvs. SwinIR-UPos), we employ an image comparison slider.\nThis tool places two images on top of each other, and al-\nlows volunteers to use a slider to alternate between them\n(see Fig. 7 for a visualization). The order of presentation of\nthe two methods (left vs. right) is randomized to eliminate\nbias. For each individual, we obtain a single score, which\nis the percentage of the time that they prefer our method\n(across those 25 images). The average score across raters\nis 69.7% (median: 68.0%; empirical standard error of the\n16\n"
    },
    {
      "page_num": 17,
      "text": "32.77\n65.83\n70.61\n73.07\n74.91\n26.45\n45.26\n45.60\n46.71\n53.20\n30.75\n48.81\n50.42\n53.28\n54.70\nLR\nOriginal GT\nSwinIR-UPos\nSwinIR-AMO\nSwinIR-AMO+FT\n35.03\n48.67\n50.48\n57.30\n65.22\n47.17\n71.16\n71.10\n74.23\n76.27\n41.69\n68.72\n72.06\n73.53\n74.73\nLR\nOriginal GT\nRESRGAN-UPos\nRESRGAN-AMO\nRESRGAN-AMO+FT\nFigure 6. Qualitative results with NR-IQA guidance. Following the notation of Table 5, columns 3-5 are (top 3 rows) SwinIR-UPos,\nSwinIR-AMO, and SwinIR-AMO + FT, and (bottom 3 rows) Real-ESRGAN-UPos, Real-ESRGAN-AMO, and Real-ESRGAN-AMO +\nFT. We show MUSIQ scores in insets. Qualitatively, we see improved performance as we move across the ‘UPos’, ‘AMO’, and ‘AMO-FT’\nmethods, particularly in terms of sharpness and detail generation. Zoom in for details.\n17\n"
    },
    {
      "page_num": 18,
      "text": "Model\nFR Low-Lev. Dist.\nFR Mid-Lev. Dist.\nNR High-Lev. Perceptual Quality\nPSNR ↑\nSSIM ↑\nLPIPS ↓LPIPS-ST ↓DISTS ↓\nMUSIQ ↑NIMA ↑Q-Align ↑\nTOPIQ ↑\nSwinIR-OrigsOnly\n✓\n26.05\n0.746\n0.37\n0.38\n0.20\n31.37\n4.24\n2.88\n0.23\nSwinIR-UPos∗\n✗\n26.02\n0.747\n0.35\n0.37\n0.20\n33.69\n4.31\n2.95\n0.24\nSwinIR-AMO\n✓\n25.99\n0.747\n0.34\n0.37\n0.19\n34.86\n4.32\n2.96\n0.25\nSwinIR-AMO + FT\n✓\n25.96\n0.742\n0.33\n0.35\n0.19\n39.25\n4.37\n2.99\n0.30\nRESRGAN-OrigsOnly\n✓\n25.90\n0.758\n0.27\n0.27\n0.16\n46.11\n4.80\n3.40\n0.32\nRESRGAN-UPos∗\n✗\n25.45\n0.750\n0.28\n0.26\n0.17\n52.74\n4.95\n3.53\n0.41\nRESRGAN-AMO\n✓\n25.22\n0.745\n0.28\n0.25\n0.17\n54.73\n4.97\n3.57\n0.45\nRESRGAN-AMO + FT\n✓\n24.71\n0.718\n0.32\n0.24\n0.19\n65.12\n5.03\n3.77\n0.63\nTable 8. Additional evaluation on the RealSRv3 [9]. Following Table 7, we evaluate the four main models on the RealSR V3 dataset,\nwhich consists of 100 test images captured using two DSLR cameras (Canon 5D3 and Nikon D810). Our methods (“AMO” and “AMO +\nFT”) achieve the highest no-reference perceptual metric (i.e., NR-IQA) scores, outperforming both “OrigsOnly” (without enhanced GT)\nand “UPos” (the SOTA baseline from HGGT, marked by ∗).\nFigure 7. User study example. Users can move the slider to alter-\nnate between 2 images.\nmean: 4.8%), suggesting our algorithm is preferred over the\nHGGT-based UPos approach at a more than 2:1 ratio, de-\nspite their use of human annotations, which ours does not\nuse. Following similar image quality assessment protocols\n(e.g., [80]), a simple single-sample one-sided t-test finds the\nrater mean significantly above 50% (p < 0.01; 95% confi-\ndence interval: [60.2%, 79.1%]).\n13. Remark on Evaluation Metric Types and\nNomenclature\nThe perception-distortion tradeoff [5] necessitates a com-\nplex suite of evaluation metrics that consider different as-\npects of the SR outputs, including pixel-level fidelity to a\nGT image and standalone image quality. Some works (e.g.,\n[90]) even utilize performance on downstream vision tasks\n(e.g., detection or segmentation) as a form of checking se-\nmantic preservation. In this work, we therefore also include\na continuum of metrics, which we hope will cover various\npoints along the perception-distortion frontier. These met-\nrics are often categorized along two different axes: (i) the\nuse of a reference and (ii) the level of visual abstraction\n(low vs mid vs high).\nNR vs FR. The first form of metric categorization is full-\nreference (FR) vs no-reference (NR). In general, FR metrics\n(which have access to a GT) measure distortion, while NR\nmetrics (which do not use a GT) measure perceptual quality.\nFor NR metrics, there is no way to measure distortion; how-\never, there are many different aspects of perceptual qual-\nity that can be considered, ranging from simple sharpness\nto differentiating aesthetic vs technical quality (e.g., [87]).\nHence, it is common (e.g., [89, 90, 95]) to use a set of NR-\nIQA models, which presumably complement each other, as\nwe do (see §3 and §7.1 for the discussion behind our metric\nchoices). For FR metrics, there is more of a spectrum (i.e.,\nthey can include some aspects of perceptual information, in\naddition to measuring distortion). PSNR and other per-pixel\ndistances have no notion of perception, operating directly\non pixel values. SSIM is meant to be more perceptual, but is\na simple, hand-crafted similarity operating on colours, lim-\niting its perceptual modelling capabilities [60]. In contrast,\nLPIPS and DISTS utilize neural network features, aiming\nto capture certain aspects of human vision. They are there-\nfore more perceptual than, e.g., PSNR, as they will toler-\nate some pixel differences (distortion) if they improve net-\nwork activation similarity. Even further along this curve\ntowards greater perceptual sensitivity is LPIPS-ST, a model\ndesigned specifically to ignore small spatial shifts (which\nare devastating to pixel-level distortion measures). Indeed,\nin many cases, we find that LPIPS-ST actually agrees with\nthe NR-IQA perceptual metrics more closely than LPIPS or\nDISTS, despite being an FR metric. Hence, FR metrics can\noccupy a range across the perception-distortion curve.\n18\n"
    },
    {
      "page_num": 19,
      "text": "Abstraction Level. A separate nomenclature arises based\non the type of information that impacts the model. It is\nbased on the hierarchical nature of biological vision (e.g.,\n[64]), but is also commonly used throughout computer vi-\nsion (e.g., [25]). Specifically, we divide visual processes\ninto low-level, relating to raw colours and 2D geometry\n(e.g., edges); mid-level, encompassing “groupings” of more\nbasic features into patterns and textures, as well as local\n3D structures; and high-level, pertaining to semantics (e.g.,\nscene classification) and representational abstraction (e.g.,\nholistic interpretations of the image). For this reason, we\nrefer to PSNR and SSIM, which operate directly on colours,\nas low-level, while LPIPS and DISTS are mid-level, as they\nrespond best to textures, image “styles”, and other regional\n“grouped” visual elements. We label neural NR-IQA mod-\nels, such as MUSIQ, as high-level, as they process the im-\nage holistically, taking semantic context into account, as\nwell as aesthetics, though they may also care about low-\nlevel issues, such as noise and blur. In general, including\nin our work, low-level metrics tend to measure distortion,\nwhile mid-level and high-level ones are more related to per-\nceptual quality. However, there may be exceptions: for in-\nstance, measuring sharpness via a simple image filter is a\nlow-level NR metric that targets perceptual quality rather\nthan distortion (e.g., [78]).\n14. Limitations\nWhile our IQA-based method is able to sharpen SR out-\nputs, as well as hallucinate aesthetically pleasing details in\nmost cases, there are still several shortcomings to our ap-\nproach. First, higher IQA model score does not guarantee\nimproved human perceptual quality nor does it strictly en-\nsure our outputs are artifact-free. This is related to the dis-\ncussion in §4.3 and Fig. 3, where we postulate that some\nimage changes can improve IQA score despite worsening\nperceptual quality (e.g., direct optimization being similar to\nan adversarial attack on the quality model). In Fig. 8, row\ntwo, for instance, we see that the SR model fails to predict\nthe correct image details, leading to incorrect line orienta-\ntions and aliasing-like artifacts (though the UPos baseline in\ncolumn two arguably has worse artifacts). Second, from a\nsemantic perspective, certain classes of image content may\nrequire different treatment, the requirements of which NR-\nIQA models are not naturally aware. For example, row one\nin Fig. 8 demonstrates how super-resolved text can become\nmangled. In terms of human preference, it can be argued\nthat having a blurrier output in such uncertain cases may be\nmore desirable (i.e., having blurred characters, rather than\nwrong characters, could be preferred for text). Nevertheless,\ntext is notoriously challenging to super-resolve (prompting\ndevelopment of specialized methods for it [49, 103]); fur-\nther, the UPos baseline suffers from similar artifacts as our\noutputs. Overall, we suspect better IQA models or more so-\n48.88\n61.00\n75.24\nOriginal GT\nRESRGAN-\nUPos\nRESRGAN-\nAMO+FT\n73.50\n76.39\n77.70\nOriginal GT\nSwinIR-UPos\nSwinIR-\nAMO+FT\nFigure 8. Illustration of limitations. We show examples of short-\ncomings of our method (see Fig. 14), with MUSIQ scores in insets.\nIn row one, we show the shortcomings of our model with respect\nto text, a particularly difficult form of image content. In row two,\nwe see that our model does still incur artifacts, such as the man-\ngled lines in the zoomed inset.\nphisticated regularized optimizations (i.e., beyond LoRA)\ncan mitigate some of the artifacts incurred by our approach.\nHandling more semantic issues, such as text hallucination,\nmay require more specialized models.\n19\n"
    }
  ]
}