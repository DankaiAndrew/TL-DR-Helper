{
  "title": "brainmvp",
  "pages": [
    {
      "page_num": 1,
      "text": "PREPRINT\n1\nBrainMVP: Multi-modal Vision Pre-training for\nBrain Image Analysis using Multi-parametric MRI\nShaohao Rui, Lingzhi Chen, Zhenyu Tang, Lilong Wang, Mianxin Liu, Shaoting Zhang, Xiaosong Wang\nAbstract— Accurate diagnosis of brain abnormalities is\ngreatly enhanced by the inclusion of complementary multi-\nparametric MRI imaging data. There is significant poten-\ntial to develop a universal pre-training model that can be\nquickly adapted for image modalities and various clinical\nscenarios. However, current models often rely on uni-modal\nimage data, neglecting the cross-modal correlations among\ndifferent image modalities or struggling to scale up pre-\ntraining in the presence of missing modality data. In this\npaper, we propose BrainMVP, a multi-modal vision pre-\ntraining framework for brain image analysis using multi-\nparametric MRI scans. First, we collect 16,022 brain MRI\nscans (over 2.4 million images), encompassing eight MRI\nmodalities sourced from a diverse range of centers and\ndevices. Then, a novel pre-training paradigm is proposed\nfor the multi-modal MRI data, addressing the issue of\nmissing modalities and achieving multi-modal information\nfusion. Cross-modal reconstruction is explored to learn\ndistinctive brain image embeddings and efficient modality\nfusion capabilities. A modality-wise data distillation module\nis proposed to extract the essence representation of each\nMR image modality for both the pre-training and down-\nstream application purposes. Furthermore, we introduce a\nmodality-aware contrastive learning module to enhance the\ncross-modality association within a study. Extensive exper-\niments on downstream tasks demonstrate superior perfor-\nmance compared to state-of-the-art pre-training methods\nin the medical domain, with Dice Score improvement of\n0.28%-14.47% across six segmentation benchmarks and a\nconsistent accuracy improvement of 0.65%-18.07% in four\nindividual classification tasks.\nIndex Terms— Multi-parametric MRI, Data distillation,\nCross-modal reconstruction, Contrastive learning\nI. INTRODUCTION\nM\nULTI-PARAMETRIC MRI (mpMRI) images combine\nvarious imaging modalities to comprehensively depict\nthe structural and pathological features of the brain [42].\nThis approach substantially enhances diagnostic accuracy and\nthoroughness [40]. For instance, in the subregional segmenta-\ntion of brain tumors, different MRI modalities reveal distinct\nlesion characteristics. Areas with high signal intensity in T1Gd\nShaohao Rui and Lingzhi Chen contributed equally to this work.\nCorresponding authors: Xiaosong Wang.\nThe authors are with Shanghai AI Laboratory, Shanghai 200030,\nChina. Shaohao Rui and Zhenyu Tang are interns at Shanghai AI\nLaboratory. (email: {ruishaohao, chenlingzhi, tangzhenyu1, wanglilong,\nliumianxin, zhangshaoting, wangxiaosong}@pjlab.org.cn)\nShaohao Rui and Zhenyu Tang are with the School of Electronic\nInformation and Electrical Engineering, Shanghai Jiao Tong University,\nShanghai 200240, China\nimages compared to T1 images and healthy white matter\nidentify the enhancing tumor (ET), whereas the tumor core,\nincluding the ET and necrotic portion, appears with low\nsignal intensity in T1Gd images relative to T1 images. The\nwhole tumor encompasses the tumor core and the surrounding\nedematous or invaded tissue, characterized by the abnormally\nhigh signal intensity in T2-FLAIR images [4].\nCurrent methods for lesion delineation and disease classifi-\ncation using mpMRI heavily rely on supervised models trained\non specific datasets, which limits their applicability across\ndifferent datasets and tasks. Developing a multi-parametric\nbrain MRI foundation model capable of addressing cross-\nmodal representation and improved performance is crucial for\nadvancing medical applications.\nHowever, challenges like the high cost of medical image\nannotation, the limited accessibility of large amounts of multi-\nmodal data [45], and the lack of dedicated multi-modal pre-\ntraining paradigm with medical image domain expertise [3]\ncomplicate the construction of such a generalized mpMRI\nfoundation model.\nObtaining a comprehensive set of modalities in mpMRI\nscans can be challenging due to the complexity associated\nwith acquisition protocol and limitations in equipment capa-\nbilities. This often leads to mismatched modality data, e.g.,\nmissing modalities cross datasets, especially when the scale of\nthe data amount increases dramatically. Furthermore, current\napproaches to dealing with missing modalities primarily focus\non particular downstream tasks, e.g., BraTS, and have not\nundergone extensive investigation in large-scale cross-modal\npre-training and downstream tasks [57, 14, 48, 25, 39].\nExisting research predominantly focuses on monotone im-\nage modality and utilizes cross-modal prompt learning by\naligning text-based information with medical images [11, 47,\n54]. However, these approaches do not directly address the\nchallenge of effectively fusing multi-modal image information\nfor mpMRI images. While some studies have begun exploring\nthis issue in cross-modal mpMRI scenarios, their methodolo-\ngies are often restricted to a small number of modalities and\nsmall datasets in either pre-training and downstream tasks [52,\n45, 31, 41], thus leading to limited model generalizability.\nMaximizing the capability of pre-training models in down-\nstream tasks holds paramount importance ([52, 44]) and is\nclosely related to the designed pre-training tasks. Frequently,\npre-trained proxy tasks lack direct correlations to downstream\napplications, e.g., masked image modeling, resulting in sub-\noptimal performance when blindly applied. By strategically\narXiv:2410.10604v1  [cs.CV]  14 Oct 2024\n"
    },
    {
      "page_num": 2,
      "text": "2\nPREPRINT\nincorporating these links, we can guide and enhance the\nefficacy of the pre-training process, ensuring that learned\nrepresentations align more closely with the requirements of\nmulti-modal fusion into the unified diagnosis.\nIn this paper, we introduce BrainMVP, a novel multi-\nmodal vision pre-training framework for the multi-parametric\nMRI images of the brain that demonstrates distinctive and\ngeneralizable cross-modal image representation. Initially, we\ngather a dataset of 16,022 publicly available brain mpMRI\nscans from various multi-center, multi-device sources. The\ndataset covers different types of brain imaging modalities,\nincluding diseased and healthy brains.\nTo address the issue of insufficient scalability (due to\nmismatched or missing modalities), we propose using single-\nmodal MRI image inputs instead of fixed modality numbers\nin the pre-training stage. This allows for the inclusion of\narbitrary numbers of modalities in the pre-training, signifi-\ncantly expanding the magnitude of available pre-training data.\nImportantly, we propose cross-modal image reconstruction via\nmask modeling. A key aspect of this design is the observation\nthat different MRI modalities for the same subject often\nexhibit significant similarity in anatomy. By employing cross-\nmodal reconstruction, we encourage the model to learn the\ndisentanglement across modalities while mining the modality-\ninvariant representations.\nToward a more generalizable pre-training model for down-\nstream tasks, we extract condensed representations of different\nmodality structures using modality-wise data distillation. Our\napproach is inspired by the technique of data distillation,\nwhich involves learning a small synthetic dataset. The per-\nformance achieved by the model training on this synthetic\ndataset can rival that achieved on the original large-scale\ndatasets [49, 58, 55]. The learned synthesized dataset indeed\nencapsulates dense representations of the original dataset.\nIn a similar idea, we optimize a set of learnable modality\ntemplates tailored for each individual modality. Intuitively,\nthe distilled modality templates retain rich structural and\nstatistical information about a specific modality while avoiding\nprivacy leakage concerns associated with individual patients.\nMoreover, the learned distilled modality templates can serve as\na linkage of data between pre-training and downstream tasks,\ni.e., as a form of information to carry and adapt between the\ndata domains for downstream applications.\nIn summary, our contributions are three fold:\n• To the best of our knowledge, BrainMVP is the first\nmulti-modal vision pre-training paradigm that aligns the\nfeatures across modalities, targeting distinctive modality-\naware representations. We collect a dataset of 16,022\nmpMRI scans (over 2.4 million images) to facilitate the\npre-training, covering a wide range of MRI brain image\nsequences in both diseased and healthy populations.\n• We design two novel proxy task settings for the multi-\nmodal vision pre-training, i.e., cross-modal reconstruction\nand cross-modality contrastive learning. To improve the\ngeneralization for downstream tasks, we also introduce\nmodality-wise data distillation to extract the template\nof each modality, benefiting both the pre-training and\ndownstream tasks.\n• We demonstrate the superior performance gain and the\nenhanced generalizability by utilizing our BrainMVP pre-\ntrained models on ten public segmentation and classifica-\ntion benchmarks, compared to state-of-the-art methods.\nII. RELATED WORK\nA. Multi-modal Pre-training for Natural Image Analysis\nIn the pursuit of acquiring knowledge, humans typically\nengage with data from multiple modalities. These diverse data\nsources, obtained from various perspectives, complement one\nanother, enabling a more comprehensive understanding and\nfacilitating the completion of more advanced semantic tasks.\nRecently, research in visual-language pre-training has seen\nsignificant advancements, primarily aiming to enhance the per-\nformance of various downstream related to vision through the\nalignment of different modal data. CLIP [36] pioneered large-\nscale image-text feature alignment by employing contrastive\nlearning to maximize the mutual information between matched\nimage-text pairs while minimizing it for mismatched pairs.\nSubsequent improvements, as noted in works like [35, 15,\n30], have demonstrated robust generalization and zero-shot\nreasoning capabilities achieved through cross-modal knowl-\nedge alignment.\nAnother type of multi-modal pre-training focuses on the\nfusion of different modal information to enhance cross-modal\ndata understanding and address the limitations of feature\nrichness in uni-modal data. For instance, [2] employs a gated\ncross-attention mechanism to integrate features extracted from\na frozen vision encoder and language model. To reduce\nthe cost of end-to-end visual-language pre-training on large-\nscale datasets, BLIP-2 [28] proposes leveraging off-the-shelf\nfrozen pre-trained image and language models. It introduces\na lightweight, learnable Q-Former module to bridge the gap\nbetween modalities, facilitating image-to-text transformation\nthrough a two-stage learning process. ALBEF [27] suggests\naligning visual and text features before inputting them into\na multi-modal Transformer network. Additionally, to enable\nefficient learning from noisy web data, ALBEF [27] introduces\na momentum distillation method to aid model training.\nWhile the aforementioned works focus on cross-domain\ninteractions, our research centers on multi-modal integration\nwithin a single domain, specifically developing pre-training\nmethods for the fusion of different modalities in MRI images.\nB. Self-Supervised Learning for Medical Image Analysis\nMedical image data annotation is notably expensive, making\nself-supervised learning (SSL) a promising avenue for the\ndevelopment of efficient annotation techniques. Given the\ntypically limited datasets available for specific medical tasks,\npre-training on large-scale unlabeled data to extract highly\ngeneralizable representations is emerging as a new paradigm.\nExisting SSL methods related to medical images can be\nroughly divided into cross-domain and in-domain fashion.\n1) Cross-domain SSL: Typical multi-modal medical image\nself-supervised pre-training is achieved through the joint in-\nvolvement of images and text. MGCA [46] leverages the\n"
    },
    {
      "page_num": 3,
      "text": "AUTHOR et al.: TITLE\n3\nsemantic correspondence between medical images and radi-\nology reports across three distinct levels: pathology region\nlevel, instance level, and disease level to facilitate generalized\nmedical visual representation learning. Additionally, a multi-\nmodal approach [42] introduces a multi-modal puzzle task\ndesigned to enhance rich representation learning from various\nimage modalities. By obfuscating image modalities at the data\nlevel and employing the Sinkhorn operator to frame the puzzle\nsolution as a permutation matrix inference, this method effi-\nciently addresses multi-modal jigsaw puzzles of varying com-\nplexity. Furthermore, [10] propose a self-supervised learning\nparadigm for medical images and texts, named the multi-modal\nmasked self-coder. This method acquires cross-modal domain\nknowledge by reconstructing missing pixels and tokens in\nrandomly masked images and texts.\n2) In-domain SSL: Most current SSL methods specific to\nmedical images are based on contrastive learning and masked\nimage modeling (MIM) to extract useful information within\nimages. For instance, [20] introduces a geometric visual simi-\nlarity pre-training framework that leverages the high topolog-\nical similarity of medical images. This approach incorporates\na priori information about topological invariance into the\nsimilarity metric between images and employs a proposed\nz-matching head to learn the similarity of semantic features\nat different scales. PCRLv2 [56] addresses the issue of local\ninformation loss in medical images within the contrastive\nlearning SSL paradigm by suggesting pixel recovery and\nfeature alignment at various scales for diverse enhancement\nsamples. Additionally, PCRLv2 [56] recommends implement-\ning SSL without using skip connections to avoid shortcut so-\nlutions in pixel restoration. SwinMM [50] trains several proxy\ntasks involving masked multi-view observation, such as image\nreconstruction, rotation, contrastive learning, and a novel task\nthat exploits the consistency of multiple views to extract\nhidden multi-view information in 3D medical data. During\nthe fine-tuning stage, SwinMM [50] utilizes cross-attention\nblocks to aggregate multi-view information. Leveraging the\nhigh structural similarity of medical images, TransVW [16]\nconceptualizes subregions of an image as transferable visual\nwords and learns generic visual representations by predict-\ning and reconstructing their region categories. Specifically,\nTransVW [16] identifies similar samples of the current im-\nage through nearest-neighbor classification on encoded image\nfeatures and uses the four regions of these similar sam-\nples for region categorization to ensure semantic consistency.\nTransVW [16] also applies perturbations to transform the\nvisual word parts, and reconstructs the transformed image to\nlearn structured semantic representations.\nOur work integrates contrastive learning and MIM based\nSSL methods, and we have deviated from previous MIM based\napproaches by discarding masked portions or filling them\nwith noise or zero values. Instead, we use informative images\nfrom other modalities for filling, which improves the learning\nefficiency and highlights the correlations between modalities.\nC. Data distillation\nData distillation is first inspired by knowledge distillation,\nproposed to distill the dataset in order to construct a core\nsubset of the data, the model is trained on the core subset\nto achieve a performance comparable to that of the complete\ndata [49]. In this way, model training and data storage costs\ncan be significantly reduced [55]. Several studies have already\napplied data distillation in the field of continual learning to\nachieve data replay to attenuate the significant oblivion of\ndistributional bias on the knowledge capability of models.\nInspired by this, we propose to use data distillation to pre-\nserve the structural statistical information of different models\nfrom pre-trained data, and to construct upstream-downstream\nlinkage through modality-wise data distillation.\nOverall,\nwe\npropose\ncross-modal\nreconstruction\nand\nmodality-aware contrastive learning as the two main proxy\ntasks, as well as enhancing the network’s learning for dis-\ncriminative information through contrastive learning. Different\nfrom previous methods, we propose a novel masking strategy\nto learn efficient modal fusion capability through cross-modal\nreconstruction and build upstream and downstream correla-\ntions through data distillation to better adapt the pre-trained\nmodels to the downstream tasks.\nIII. MULTI-MODAL VISION PRE-TRAINING\nAs shown in Fig. 1, BrainMVP consists of three proxy tasks:\ncross-modal reconstruction, modality-wise data distillation,\nand modality-aware contrastive learning. The proposed cross-\nmodal reconstruction (via two ways of masking and mix)\nmodule aims to achieve the disentanglement across modalities\nwhile mining the modality-invariant representations. Modality-\nwise data distillation is designed to learn compressed structural\ninformation for each modality from pre-trained unlabeled data\nwhile allowing the model to extract modality-wise information\nand learn modality-agnostic features. Furthermore, the distilled\nmodality templates are applied to downstream tasks to estab-\nlish the association of data between the pre-training and down-\nstream domains, which helps to improve the generalization\nperformance of the foundation model. Finally, modality-aware\ncontrastive learning is integrated to ensure the consistency of\nsemantic features between different masked versions of the\nsame sample, as well as to extract discriminative information\nbetween modalities.\nA. Cross-Modal Reconstruction\nProblem Setting. Given an unlabeled dataset D = {Xim ∈\nRD×H×W |m ∈{1, . . . , Mi}, i ∈{1, . . . , N}}. Mi denotes\nthe number of modalities in i-th sample and N represents\ntotal number of samples. Masked image modeling (MIM) first\nmasks (with constant values, zeros, or noise, etc, denoted\nas Φ(·) ) a large portion of Xim to obtain a masked input\nΦ(Xim), and then reconstructs the original image from it\nto learn efficient representations. Specifically, let the encoder\nand decoder of the model be fenc(·) and fdec(·) respectively,\nwhere the model f(·) is the composition of the encoder and\ndecoder functions, i.e., f(·) = fdec◦fenc. MIM minimizes the\nfollowing reconstruction loss:\nLrec = 1\nN\nN\nX\ni=1\n1\nMj\nMj\nX\nm=1\n||fdec(fenc(Φ(Xim)) −Xim||2\n(1)\n"
    },
    {
      "page_num": 4,
      "text": "4\nPREPRINT\nFig. 1.\nOverview of the proposed BrainMVP, comprised of (a) cross-modal reconstruction module that aims at learning a mapping from images\nmasked and mixed with another modality to the original; (b) modality-wise data distillation module that learns condensed modality templates via\ngradient backpropagation; and (c) modality-aware contrastive learning module for introducing case-level modality invariance to the learned features.\nThe core idea of our proposed reconstruction proxy tasks,\nwhich are elaborated in Sections III-A and III-B, is to obtain\nmeaningful representations via exploiting different forms of\nΦ(·) function.\nPixel-level cross-modal masking. Given a uni-modal input\nvolume Xim sampled from a mpMRI case (with Mi modal-\nities), cross-modal masking aims to mask a large region of\nXim with another modality image Xin (also sampled from Xi,\nn ̸= m). Specifically, we first randomly mask a region of size\nr × r × r in Xim, where r denotes the size of each dimension\nof input 3D volumes. Then, we fill the masked region with\na patch cropped with the same location and size on another\nmodality of the sampled case. Finally, we repeat the above\nmasking-filling operation until the proportion of masked pixels\nover the total input volume (Xim) pixels arrives p∗. Referring\nto [31], we empirically set r = 8 and p∗= 0.8 to learn useful\nrepresentations. Details can be seen in Algorithm 1.\nAlgorithm 1 Pixel-level cross-modal masking.\nSample randomly Xim from Xi\nSample randomly Xin(n ̸= m) from Xi\nptotal ←H × W × D\npmask ←0\nwhile pmask < ptotal × p∗do\nSelect randomly (x, y, z) in Xim\nMask an area of size r × r × r centered at (x, y, z)\nFill with corresponding data from Xin\npmask ←pmask + r × r × r\nend while\nreturn modified Xim\nCross-modal reconstruction. Let our proposed cross-\nmodal masking strategy be Φmodal(·). Given that the masking\noperation masks a large portion of the image, the resulting\nmasked input volume Φmodal(Xim) will contain informa-\ntion predominantly from Xin. The extracted representation\nfenc(Φmodal(Xim)) will thus encode a significant amount\nof semantic information from Xin. Since we do not in-\ntroduce skip connections between the encoder and decoder,\nwe only reconstruct Xim from the latent representation\nfenc(Φmodal(Xim)), which is a challenging task for natural\nimages. However, due to the high structural similarity between\ndifferent modalities in mpMRI images, with strong contrasts\nonly in certain regions, the cross-modal reconstruction can\nencourage the model to learn cross-modal representations\nand explore the correlations between different modalities.\nFurthermore, Φmodal(Xim) still contains (1−p∗) proportion of\ninformation about Xim, and reconstructing this part will retain\nsome original modality information, which can help reduce the\ndifficulty of pure cross-modal reconstruction and extract the\nsemantic information of the Xim image itself. Formally, the\ncross-modal reconstruction loss can be expressed as:\nLCMR = 1\nN\nN\nX\ni=1\n1\nMj\nMj\nX\nm=1\n||fdec(fenc(Φmodal(Xim)) −Xim||2\n(2)\nB. Modality-wise Data Distillation\nThe primary objective of the foundation model is to extract\nhighly generalizable latent representations. However, the proxy\ntasks currently used in pre-training models are often unrelated\nto the downstream application tasks. We attempt to introduce\ncertain bridging components during the model pre-training\nstage that can guide the pre-training process to acquire the nec-\nessary specific representations. Simultaneously, we hope that\nthese bridging components can facilitate the feature expression\nof the pre-trained model when applied to downstream tasks. As\nshown in Fig. 1, the proposed modality-wise data distillation\nis in conjunction with the cross-modal reconstruction process.\nSpecifically, in the cross-modal reconstruction part, we use\ndata either from another modality image Xin to fill in the\nmasked region in Xim or from the corresponding learnable\nmodality template.\nSpecifically,\nthe\nlearnable\nmodality\ntemplates\nT\n=\n{Tm}S\nm=1 sized S×H×W ×D are initialized with zero, where\nS represents the number of modalities in the pre-training\ndatasets. Similar to cross-modal reconstruction, the image\nneeded for filling Xim is Tm (m represents the corresponding\nmodality) instead of another modality in the modality-wise\ndata distillation process. The remaining steps are consistent\nwith the cross-modal reconstruction process. An example of\nlearned modality templates is shown in the Results section\n"
    },
    {
      "page_num": 5,
      "text": "AUTHOR et al.: TITLE\n5\n(Fig. 4), which shows a compact representation of the struc-\ntural information for each modality in the pre-training datasets.\nIt is worth noting that the random masked regions are\nused in the two processes for the same input, which further\naccelerates the learning. Let us denote the masking strategy\nfor modality-wise data distillation as Φdistill, and the corre-\nsponding loss can be expressed as:\nLMD = 1\nN\nN\nX\ni=1\n1\nMj\nMj\nX\nm=1\n||fdec(fenc(Φdistill(Xim)) −Xim||2\n(3)\nCross-modal reconstruction and modality-wise data distilla-\ntion are performed simultaneously. The model needs to learn\nnot only the structural information of a specific modality to\nform the distilled modality templates but also the transfor-\nmation relationship between modalities. The representations\nlearned by our pre-trained model are considered modality-\nagnostic and contain fused representations of different modal-\nities.\nC. Modality-aware Contrastive Learning\nDuring the pre-training, we also need to consider the data\ndistribution bias across different datasets due to the variances\nin MRI imaging equipment, acquisition protocols, etc. It could\nlargely enhance the generalization of pre-trained models in the\ndownstream tasks when unseen data are applied.\nInspired by the use of contrastive learning for aligning\nthe paired features in multi-modal pre-training schemes, e.g.,\nInfoNCE loss in CLIP, we first compose the positive and\nnegative pairs. A sample masked and mixed with another\nmodality together with another sample masked and mixed with\nthe corresponding modality template forms a positive pair.\nSimilarly, a sample masked and mixed with another modality\ntogether with another sample (from different datasets) masked\nand mixed with the corresponding modality template forms a\nnegative pair. In such a way, the model tends to learn both\ndataset- and modality-independent features.\nWe denote the sets obtained by encoding N data sam-\nples using the aforementioned two masking strategies as\n{Φmodal(Xim)}N\ni=1 and {Φdistill(Xim)}N\ni=1, respectively. As\nshown in Fig. 1, We use contrastive loss [36] to bring\nthe distance between features of positive pairs closer while\nrepelling the distance between features of negative pairs. This\ncan be formalized as:\nLCL = −1\n2N\nN\nX\ni=1\n\n\n\n\nlog\nefi·gi/τ\nkP\nj=0\nefi·gj/τ\n+ log\negi·fi/τ\nkP\nj=0\negi·fj/τ\n\n\n\n\n\n(4)\nwhere fi=fenc(Φmodal(Xim)) represents current modality\nimage masked and mixed with another modality image in\nthe same sample, gi=fenc(Φdistill(Xim)) represents current\nmodality image masked and mixed with corresponding dis-\ntilled modality template. gj,j̸=i represents modality images\nfrom other samples (datasets) masked and mixed with cor-\nresponding modality templates. N is the number of samples\nand τ is the distillation temperature.\nOverall loss function. In summary, the total loss function\nis a combination of LCMR, LMD, and LCL:\nL = LCMR + λMD · LMD + λCL · LCL\n(5)\nwhere λMD as well as λCL are used to balance the corre-\nsponding loss term contributions, which are both set to 1.0 in\nthe experiments for equal treatment.\nD. Distilled Modality Template for Downstream Tasks\nIn this section, we will elaborate on how the distilled modal-\nity templates obtained from pre-training can be applied in\ndownstream tasks, as a form of data augmentation. As shown\nin Fig. 2, in the downstream fine-tuning stage, the distilled\nmodality templates are frozen. Let Dds = {(Xi, Yi)}M\ni=1\ndenote the downstream dataset, where M represents the\nnumber of annotated samples. Xi is the multi-modal MRI\ninput volume, and Yi represents the corresponding label,\nwhich can be a segmentation map for segmentation tasks\nor a one-hot vector for classification tasks. Specifically, we\nrandomly select n modalities in Xi and replace them with\nthe corresponding modalities from {Tm}S\nm=1, obtaining two\naugmented copies X′\ni and X′′\ni . The encoded features of these\ntwo copies are fenc(X′\ni) and fenc(X′′\ni ), respectively. Since the\ntwo embeddings are representations of the same sample with\ndifferent numbers of replaced modalities, we use the L2 norm\nto maintain semantic consistency in the feature space.\nLcons = 1\nN\nN\nX\ni=1\n||fenc(X′\ni) −fenc(X′′\ni )||2\n(6)\nSubsequently, the features of the two copies are decoded to\nthe output space to calculate supervision loss with the ground-\ntruth annotations. The overall loss is:\nLtotal = 1\nN\nN\nX\ni=1\nLsl(f(X′\ni), Yi) + Lsl(f(X′′\ni ), Yi) + λcons ∗Lcons\n(7)\nwhere λcons is the weight of the consistency loss Lcons\nterm and Lsl is the supervision loss used in segmentation\nor classification tasks, e.g., Dice Loss in segmentation or\nCrossEntropyLoss in classification.\nFor the uni-modal input scenario, instead of replacing the\nselected modalities with distilled modality templates, we per-\nform a partially masking strategy like Algorithm 1 where Xin\nis replaced with the corresponding distilled modality template.\nThen we randomly mask the uni-modal input volume twice\nto obtain two augmented copies of Xim, and the remaining\nprocedures are the same as the aforementioned multi-modal\nscenario.\nIV. EXPERIMENTS\nA. Datasets\nPre-training Datasets We collect five publicly available\nmpMRI datasets for pre-training, spanning 8 modalities with\na total of 3,755 cases and 16,022 scans. The task types and\nnumber of modalities for each dataset are summarized in\nTable I.\n"
    },
    {
      "page_num": 6,
      "text": "6\nPREPRINT\nFig. 2. Modality-wise data distillation for downstream tasks. The input multi-modal MRI images are randomly selected to replace a certain number\nof modalities with the corresponding modality templates. Then L2 norm is used to ensure feature consistency between the two replacement copies.\nFinally, the task head is replaced with corresponding modules based on the task type.\nTABLE I\nDETAILS OF DATASETS USED IN OUR WORK.\nDataset\nTask type\nModality type\ncases\nPre-training\n3755\nBraTS2021 [4]\n-\nT1,T1CE,T2,FLAIR\n1470\nBraTS2023-SSA [1]\n-\nT1,T1CE,T2,FLAIR\n75\nBraTS2023-MEN [26]\n-\nT1,T1CE,T2,FLAIR\n1141\nBrainAtlas [22]\n-\nT1,T2,MRA,PD\n568\n-\nUCSF-PDGM [7]\n-\nT1,T1CE,T2,FLAIR,DWI,ADC\n501\nDownstream\nBraTS-PED [24]\nseg. (pediatric tumor)\nT1,T1CE,T2,FLAIR\n99\nBraTS2023-MET [34]\nseg. (brain metastases)\nT1,T1CE,T2,FLAIR\n237\nISLES22 [21]\nseg. (ischemic stroke lesion)\nFLAIR,DWI,ADC\n238\nMRBrainS13 [33]\nseg. (CF,GM,WM)\nT1,T1CE,FLAIR\n20\nUPENN-GBM [6]\nseg. (glioblastoma)\nT1,T1CE,T2,FLAIR\n127\nVSseg [38]\nseg. (vestibular schwannoma)\nT1\n242\nBraTS2018 [5]\ncls. (HGG and LGG)\nT1,T1CE,T2,FLAIR\n285\nADNI [23]\ncls. (MCI and NC)\nT1\n1348\nADHD-200 [12]\ncls. (ADHD and NC)\nT1\n767\nABIDE-I [13]\ncls. (ASD and NC)\nT1\n819\nseg.: segmentation; cls.: classification; CF: Cerebrospinal Fluid; GM: Gray Matter;\nWM: White Matter; HGG: Higher Grade Glioma; LGG: Lower Grade Glioma; MCI: Mild\nCognitive Impairment; NC: Normal Control; ADHD: Attention Deficit Hyperactivity\nDisorder; ASD: Autism Spectrum Disorder.\nDownstream Datasets Our pre-trained models are evalu-\nated on ten downstream tasks (in a full fine-tuning setting),\nincluding six segmentation tasks and four classification appli-\ncations, as detailed in Table I.\nB. Experiments setting\nData Pre-processing. During the pre-training stage, data\npre-processing is performed sequentially in Python based on\nMONAI 1.3.01 library. The orientation of the mpMRI images\nis first unified to the RAS axcodes and co-registered to the\nsame anatomical template. Subsequently, each MRI scan is\nresampled to an isotropic voxel spacing of 1.0mm×1.0mm×\n1.0mm using bilinear interpolation, and skull-stripping is\nperformed as well. We linearly clip the pixel values between\nthe 1st and 99th percentiles and re-scale them to [0, 1]. The\nimages are then cropped into 96 × 96 × 96 voxel patches\ncentered on either foreground or background areas, to ensure\nthat the modality-wise data distillation is learned sufficiently.\nWe do not apply any other data augmentation techniques.\nImplemention details. We use UniFormer [29] for pre-\ntraining and downstream tasks, benefiting from its natural\nmulti-modal fusion capabilities. In addition, we have con-\nducted experiments related to the UNET3D [37] architecture.\nWe conduct all experiments using the PyTorch framework on\n1https://monai.io/\n8 NVIDIA GeForce RTX 4090 GPUs. During the training pro-\ncess, we utilize the AdamW [32] optimizer with a momentum\nof 0.9 and the weight decay is 1e-5. We train the model for\n1,500 epochs with a batch size of 3 and introduce the modality-\naware contrastive learning module at epoch 900. The initial\nlearning rate is set to 3e-4 and we employ a cosine learning\nrate decay strategy. Detailed hyperparameters for downstream\nexperiments can be found in the Appendix I.\nComparison methods. We compare our BrainMVP against\nthree different types of approaches, i.e., training from scratch,\ngeneral domain SSL methods, and medical domain SSL\nmethods. There are three mainstream medical image seg-\nmentation networks for training from scratch: UNETR [18],\nUNET3D [37] [37], and Swin-UNETR [17]. UniFormer [29]\nis a novel 3D medical image segmentation network initially\ndeveloped in the field of video object detection and extensive\nexperiments have been conducted to verify its effectiveness.\nThe subsequent SSL methods are pre-trained on the above\narchitectures, allowing for a fair comparison of the impact\nof different network architectures on the final performance.\nThe baseline SSL methods include MAE3D [19, 9], MIM-\nbased SimMIM [53], and contrastive learning related Mo-\nCoV3 [8] for general domain, and MG [59], TransVW [16],\nGVSL [20], Swin-UNETR [43], and VoCo [51] for medical\ndomain. Specifically, two MIM-based methods in medical\ndomain, namely, DAE [45] and M3AE [31], are also taken\nas comparisons.\nEvaluation metrics. For segmentation tasks, we use the\nDice Score and Hausdorff distance at 95th percentile (HD95)\nas evaluation metrics. For classification tasks, we report ac-\ncuracy (ACC), area under the curve (AUC), and F1 score for\ncomprehensive assessment with higher metric values indicat-\ning better classification performance.\nLabel efficiency experiments. To validate if our Brain-\nMVP, pre-trained on large-scale mpMRI image datasets, can\nsignificantly reduce annotation workload in clinical practices,\nparticularly for handling label-deficient segmentation tasks\n(which incur higher annotation costs), we conduct label effi-\nciency experiments on five segmentation and one classification\ndatasets. Specifically, we randomly split the training labeled\nsamples into five partitions and gradually increase the training\nset size by one partition at a time until reaching the full\ndataset size. The resulted experiments are configured with\n20%, 40%, 60%, 80%, and 100% of the total training data.\n"
    },
    {
      "page_num": 7,
      "text": "AUTHOR et al.: TITLE\n7\nTABLE II\nEXPERIMENTAL RESULTS ON SIX DOWNSTREAM SEGMENTATION DATASETS. WE REPORT THE MEAN DICE SCORE (%) ON EACH DATASET AND THE\nBEST RESULTS ARE BOLDED. THE SECOND BEST RESULTS ARE UNDERLINED.\nMethod\nModality\nNetwork\nBraTS-PED [24]\nBraTS-MET [34]\nISLES22 [21]\nMRBrainS13 [33]\nVSseg [38]\nUPENN-GBM [6]\nET\nTC\nWT\nAVG\nET\nTC\nWT\nAVG\nIS\nCF\nGM\nWM\nAVG\nVS\nET\nTC\nWT\nAVG\nFrom Scratch\nUNETR [18]\n-\n-\n46.46 76.43 78.66 67.19 54.01 54.87 59.44 56.11\n74.65\n67.55 78.73 83.69 76.66\n70.28\n83.10 80.88 81.98 81.99\nUNET3D [37]\n-\n-\n47.12 81.60 83.94 70.89 56.44 58.75 62.76 59.32\n80.94\n70.47 73.93 82.96 75.78\n69.43\n85.65 88.76 86.27 86.89\nUniFormer [29]\n-\n-\n46.73 83.87 86.97 72.52 67.22 72.74 70.78 70.25\n84.97\n77.66 74.09 75.60 75.78\n80.33\n87.93 91.86 88.81 89.53\nSwin-UNETR [17]\n-\n-\n49.66 81.10 84.13 71.63 63.84 67.08 68.58 66.50\n75.88\n70.35 81.66 84.65 78.89\n76.82\n87.60 91.15 87.34 88.70\nWith General SSL\nMAE3D [19, 9]\nNatural\nUNETR\n46.55 77.08 79.32 67.65 57.45 59.19 62.06 59.57\n70.43\n68.30 80.57 84.69 77.86\n69.57\n83.66 80.42 81.86 81.98\nSimMIM [53]\nNatural\nUNETR\n45.14 76.59 78.61 66.78 54.46 55.84 58.89 56.40\n69.94\n68.11 80.49 84.76 77.79\n69.08\n83.70 81.68 82.44 82.61\nMoCov3 [8]\nNatural\nUNETR\n45.66 77.37 79.88 67.64 55.84 56.77 61.62 58.07\n70.32\n67.97 79.64 84.36 77.32\n69.83\n83.02 80.54 81.77 81.78\nWith Medical SSL\nMG [59]\nCXR, CT\nUNET3D\n47.99 86.69 88.41 74.36 60.11 64.05 65.43 63.19\n83.53\n71.40 74.71 80.41 75.51\n76.33\n86.64 90.58 87.03 88.08\nTransVW [16]\nCT\nUNET3D\n46.38 80.05 81.98 69.47 56.10 58.69 62.81 59.20\n80.24\n68.92 80.53 83.70 77.72\n71.76\n85.95 89.51 86.91 87.46\nGVSL [20]\nCT\nUNET3D\n49.05 84.47 86.81 73.45 62.46 66.81 67.26 65.51\n80.05\n69.34 75.07 82.85 75.75\n72.21\n87.09 91.75 87.53 88.79\nSwin-UNETR* [43]\nMRI\nSwin-UNETR 49.07 81.74 84.13 71.65 60.60 64.56 64.53 63.23\n79.55\n69.67 82.09 86.13 79.30\n75.55\n87.24 91.46 87.28 88.66\nVoCo [51]\nMRI\nSwin-UNETR 48.66 82.26 84.64 71.85 57.49 59.33 63.59 60.13\n77.58\n71.29 76.43 81.40 76.37\n76.45\n86.65 90.54 87.34 88.18\nDAE [45]\nMRI\nSwin-UNETR 49.30 82.12 84.78 72.07 62.27 65.99 64.85 64.37\n73.92\n71.37 78.50 83.20 77.69\n74.51\n86.90 90.83 87.32 88.35\nM3AE [31]\nMRI\nUNET3D\n46.77 85.67 86.89 73.11 66.01 70.92 70.18 69.04\n83.85\n71.32 69.56 79.28 73.39\n75.96\n87.15 91.90 88.44 89.16\nM3AE [31]\nMRI\nUniFormer\n50.77 84.95 86.70 74.14 68.08 72.35 70.74 70.39\n86.32\n78.23 77.20 76.43 77.29\n79.31\n87.75 92.43 88.72 89.63\nBrainMVP\nMRI\nUNET3D\n47.75 85.99 88.46 74.07 67.24 71.27 68.63 69.05\n83.31\n68.88 74.60 82.66 75.38\n76.02\n87.30 91.87 88.98 89.38\nBrainMVP\nMRI\nUniFormer\n55.45 86.54 88.41 76.80 70.70 75.80 74.52 73.67\n86.60\n81.04 78.17 81.61 80.27\n83.64\n88.49 92.48 89.07 90.01\nCXR: Chest X-Ray; ET: enhancing tumor; TC: tumor core; WT: whole tumor; AVG:average; IS: Ischemic Stroke; CF: Cerebrospinal Fluid; GM: Gray matter; WM: White matter;\nVS: Vestibular schwannoma.\nThe validation and test sets are kept the same for a fair com-\nparison. For the comparison methods, we select representative\napproaches for each pre-training data modality (natural, CT,\nand MRI), including MAE3D [19, 9], GVSL [20], MG [59],\nand VoCo [51]. Notably, we observe that MG [59] exhibits\nstrong generalization performance across many datasets, so we\ninclude it in the label efficiency experiments to verify whether\nour method has superior performance.\nV. RESULTS\nA. Experimental Results on Ten Downstream Tasks\n1) Segmentation Results: Superior performance on tu-\nmor segmentation datasets. To evaluate the improvement\nof downstream segmentation performance after pre-training,\nwe select two brain gliomas subregion segmentation datasets,\nBraTS-PED [24] and UPENN-GBM [6]. BraTS-PED, dedi-\ncated to pediatric glioma, comprises only 99 annotated cases,\nmaking it a challenging testbed for assessing the generalization\ncapability of pre-trained foundation models. Consequently,\ninitial comparative experiments are conducted on this dataset.\nAs depicted in Table II, SSL methods tailored for medical\nimage domains consistently outperform general SSL methods.\nIt is worth noting that models pre-trained on natural image\ndemonstrate poorer generalization on medical image domains.\nSpecifically, the best average Dice Score achieved by general\nSSL methods based on MIM is 67.65%, which is 9.15% lower\nthan BrainMVP’s best result of 76.80%. Also, MoCoV3 [8]\nperforms less effectively, achieving 9.16% lower in Dice Score\ncompared to BrainMVP. This disparity arises because typical\npre-training methods developed primarily for 2D image tasks\noften require full images or large patches as input, which is\nusually impractical for 3D medical images. Our BrainMVP\nalso outperforms medical SSL methods based on mask mod-\neling, such as M3AE [31] (76.80% vs. 74.14%) and DAE [45]\n(76.80% vs. 72.07%). We further validate the effectiveness\nof BrainMVP on UPENN-GBM [6], as shown in Table II.\nBrainMVP achieves an average Dice Score of 90.01% and\noutperforms state-of-the-art methods.\nPerformance improvement on brain structure segmenta-\ntion dataset. We utilize the MRBrainS13 [33] dataset for the\nsegmentation of normal brain structures to assess the efficacy\nof BrainMVP in scenarios with limited normal brain structure\nsamples during pre-training. As detailed in Table II, our Brain-\nMVP achieves an average Dice Score of 80.27%. In contrast,\nMG [59], employing multiple proxy tasks, attains 75.51%, and\nVoCo [51], leveraging contrastive learning, achieves 76.37%.\nBased on the UniFormer [29] architecture, BrainMVP sur-\npasses all previous methods and demonstrates a notable 4.49%\naverage Dice Score improvement over training from scratch.\nThis underscores its robust capability to effectively improve\ndownstream tasks performance, even under constraints with\nlimited normal brain structure data samples in pre-training.\nStrong generalization performance on Unseen datasets.\nGiven that our pre-training datasets primarily include normal\nbrain structures and those afflicted with glioma, we aim to ver-\nify the generalization capabilities of BrainMVP on other types\nof samples. To assess this, we evaluate our BrainMVP on three\ndatasets: BraTS-MET [34], ISLES22 [21], and VSseg [38].\nFor the BraTS-MET [34] dataset focusing on brain metastasis\nsubregion segmentation, as seen in Table II, our BrainMVP\nachieves an average Dice Score of 73.67%. Further, Brain-\nMVP notably outperforms existing state-of-the-art methods in\nmedical applications, including MG [59] (63.19%), and Swin-\nUNETR* [43] (63.23%). In the context of the ISLES22 [21]\nischemic stroke segmentation task, which involves abnor-\nmalities distinct from tumors targeted in pre-training, Brain-\nMVP achieves substantial improvement compared to MG [59]\n(86.60% vs. 83.53%) and GVSL [20] (86.60% vs. 80.05%).\nFor the VSseg [38] dataset focusing on vestibular schwannoma\nsegmentation task, in previous methods, M3AE [31] achieves\nthe best performance with 79.31% Dice Score, while our\nBrainMVP outperforms all previous methods with 83.64%\nDice Score, proving the effectiveness of BrainMVP.\n"
    },
    {
      "page_num": 8,
      "text": "8\nPREPRINT\nTABLE III\nEXPERIMENTAL RESULTS ON FOUR DOWNSTREAM CLASSIFICATION DATASETS. WE REPORT THE OVERALL ACCURACY (ACC), AREA UNDER THE\nCURVE (AUC) AND F1 SCORE ON EACH DATASET. THE BEST RESULTS ARE BOLDED AND THE SECOND BEST RESULTS ARE UNDERLINED.\nMethod\nModality\nNetwork\nBraTS2018 [5]\nADNI [23]\nADHD-200 [12]\nABIDE-I [13]\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nACC\nAUC\nF1\nFrom Scratch\nUNETR [18]\n-\n-\n0.7895\n0.7817\n0.6621\n0.5672\n0.6066\n0.5645\n0.6688\n0.6523\n0.6204\n0.6121\n0.5478\n0.5507\nUNET3D [37]\n-\n-\n0.7368\n0.7373\n0.4242\n0.5756\n0.4966\n0.3653\n0.6494\n0.6798\n0.4265\n0.6061\n0.5059\n0.4591\nUniFormer [29]\n-\n-\n0.7762\n0.7719\n0.6994\n0.5546\n0.6343\n0.5526\n0.6039\n0.6387\n0.5796\n0.5879\n0.4433\n0.4292\nSwin-UNETR [17]\n-\n-\n0.7018\n0.7143\n0.6069\n0.5672\n0.5853\n0.5650\n0.6494\n0.6950\n0.6240\n0.6121\n0.5530\n0.5596\nWith General SSL\nMAE3D [19, 9]\nNatural\nUNETR\n0.7018\n0.6754\n0.5645\n0.5756\n0.5414\n0.5651\n0.6169\n0.6489\n0.5906\n0.6061\n0.4983\n0.4591\nSimMM [53]\nNatural\nUNETR\n0.7368\n0.8349\n0.7077\n0.6218\n0.6026\n0.5446\n0.6234\n0.6567\n0.5790\n0.5394\n0.5819\n0.5318\nMoCov3 [8]\nNatural\nUNETR\n0.7368\n0.8135\n0.7304\n0.6092\n0.5769\n0.5996\n0.6104\n0.6265\n0.6007\n0.5939\n0.6284\n0.5890\nWith Medical SSL\nMG [59]\nCXR, CT\nUNET3D\n0.7368\n0.9286\n0.4242\n0.5756\n0.5496\n0.3653\n0.6169\n0.6980\n0.6141\n0.6121\n0.6266\n0.5892\nTransVW [16]\nCT\nUNET3D\n0.7368\n0.7222\n0.4242\n0.4958\n0.6661\n0.4450\n0.6818\n0.7228\n0.6271\n0.6424\n0.5292\n0.5003\nGVSL [20]\nCT\nUNET3D\n0.7895\n0.8516\n0.7286\n0.5966\n0.6661\n0.5959\n0.6623\n0.7309\n0.6565\n0.6242\n0.5244\n0.4701\nSwin-UNETR* [43]\nMRI\nSwin-UNETR\n0.7368\n0.5032\n0.4242\n0.5462\n0.5517\n0.5461\n0.6299\n0.6437\n0.5953\n0.6303\n0.4993\n0.3866\nVoCo [51]\nMRI\nSwin-UNETR\n0.7368\n0.5135\n0.4242\n0.5210\n0.5740\n0.5207\n0.6558\n0.6971\n0.6413\n0.5818\n0.5626\n0.5466\nDAE [45]\nMRI\nSwin-UNETR\n0.7719\n0.8151\n0.7120\n0.5294\n0.5666\n0.5294\n0.6688\n0.7129\n0.6548\n0.6061\n0.5173\n0.5548\nM3AE [31]\nMRI\nUNET3D\n0.7370\n0.6984\n0.5915\n0.6008\n0.6338\n0.6003\n0.6364\n0.7049\n0.6177\n0.6061\n0.5453\n0.4769\nM3AE [31]\nMRI\nUniFormer\n0.7895\n0.8659\n0.7159\n0.6092\n0.5352\n0.5756\n0.6169\n0.6597\n0.6028\n0.5636\n0.4682\n0.4500\nBrainMVP\nMRI\nUNET3D\n0.7895\n0.7746\n0.6621\n0.6555\n0.6669\n0.6421\n0.6818\n0.7245\n0.6665\n0.6970\n0.5817\n0.6327\nBrainMVP\nMRI\nUniFormer\n0.8596\n0.9452\n0.8324\n0.6765\n0.6964\n0.6609\n0.6883\n0.7249\n0.6723\n0.6182\n0.6329\n0.5890\n2) Classification Results: We select four distinct classifica-\ntion tasks to assess the generalizability of BrainMVP across\ndiverse domains. Initially, experiments are conducted on the\nBraTS2018 [5] glioblastoma grading task. As depicted in\nTable III, M3AE [31] achieves the best ACC compared with\nother methods with 0.7895, while our BrainMVP achieves an\noutstanding 0.8596, surpassing the state-of-the-art methods by\na large margin. For example, M3AE [31] achieves accuracies\nof 0.7370 and 0.7895), VoCo [51] achieves an accuracy of\n0.7368, and GVSL [20] achieves an accuracy of 0.7895.\nAdditionally, BrainMVP exhibits superior performance in F1\nscore and AUC compared to prior SSL methods, underscoring\nits efficacy.\nSubsequently, BrainMVP is validated on the ADNI [23]\ndataset to assess its ability to differentiate between healthy\nand diseased states, a task not represented in the pre-training\ndatasets. Notably, the pre-training datasets comprise only a\nsmall fraction of normal brain data (12.5%). Experimental\nresults reveal that BrainMVP displays robust generalization\ncapabilities, achieving the highest accuracy of 0.6765 with\nthe UNET3D [37] network, compared to an accuracy of\n0.5756 for training from scratch. In terms of AUC, Brain-\nMVP achieves 0.6964, a 3.03% improvement over the best-\nperforming method GVSL [20] (0.6661). Similarly, BrainMVP\noutperforms the state-of-the-art methods in F1 score, demon-\nstrating strong generalizability despite disparities between pre-\ntraining and downstream tasks.\nMoreover, to further investigate BrainMVP’s generalization\nacross diverse tasks and domains, experiments are conducted\non the ADHD-200 [12] and ABIDE-I [13] datasets. Results\nindicate that BrainMVP consistently outperforms the state-\nof-the-art SSL methods. Specifically, on the ADHD-200 [12]\ndataset, BrainMVP achieves an accuracy of 0.6883 while the\nbest result achieved in the previous method is 0.6818. On\nthe ABIDE-I [13] dataset, BrainMVP demonstrates 5.46%\naccuracy improvement, 0.45% AUC improvement, and 4.35%\nF1 score improvement, establishing BrainMVP as a robust\napproach surpassing existing SSL methods.\nIt is important to note that our BrainMVP leverages pre-\ntraining on partially relevant normal brain mpMRI images,\nfurther validating the strong generalizability and superior\nperformance of our BrainMVP.\nB. Results of Label Efficiency Experiments\nFig. 3 illustrates the results of the label efficiency experi-\nments. It can be observed that when BrainMVP is fine-tuned\non downstream segmentation and classification tasks with\nvarying ratios of labeled training data, BrainMVP consistently\nexhibits superior performance compared to the representative\nmethods. As the labeled data increase from 20% to 40%,\nBrainMVP shows a significant performance improvement\non multiple datasets, such as BraTS-PED [24] (Dice Score\n66.41% to 70.46%), BraTS-MET [34] (Dice Score 60.45% to\n70.12%), and ISLES22 [21] (Dice Score 73.27% to 84.03%).\nSimilarly, on the classification task of BraTS2018 [5], the AUC\nalso shows a substantial increase (0.6833 to 0.8008).\nIt is noteworthy that with only 40% of the labeled data,\nBrainMVP can achieve performance on par with or even\nsurpassing those of other methods using the fully labeled\ndata. With just 20% of the labeled data, BrainMVP can\nattain 66.41% Dice Score on the BraTS-PED [24] dataset,\n70.39% Dice Score on the VSseg [38] dataset, and 86.82%\nDice Score on the UPENN-GBM [6] dataset, while the corre-\nsponding best-performing methods achieve 59.50%, 52.31%,\nand 80.97%, respectively. This demonstrates the excellent\nefficiency of our BrainMVP, which can lift the annotation\nrequirements in clinical practices.\nC. Ablation Study\nWe\nconduct\ncomprehensive\nablation\nexperiments\non\nthree key components of the proposed BrainMVP: cross-\nmodal reconstruction, modality-wise data distillation, and\nmodality-aware contrastive learning on the BraTS-PED [24],\nBraTS2018 [5], and ADNI [23] datasets. The results are shown\nin Table IV.\n"
    },
    {
      "page_num": 9,
      "text": "AUTHOR et al.: TITLE\n9\n(a) BraTS-PED [24]\n(b) BraTS-MET [34]\n(c) ISLES22 [21]\n(d) VSseg [38]\n(e) UPENN-GBM [6]\n(f) BraTS2018 [5]\nFig. 3.\nLabel efficiency results of the downstream segmentation and classification tasks. We report the mean Dice Score (%) in segmentation and\narea under the curve (AUC) in classification.\nTABLE IV\nABLATION EXPERIMENTAL RESULTS ON BRATS-PED [24], BRATS2018 [5] AND ADNI [23] DATASETS.\nTask\nBraTS-PED [24]\nBraTS2018 [5]\nADNI [23]\nCMR\nMD\nMCL\nDice Score (%)\nACC\nAUC\nF1\nACC\nAUC\nF1\n%\n%\n%\n72.52\n0.7762\n0.7719\n0.6994\n0.5546\n0.6343\n0.5526\n\"\n%\n%\n75.16\n0.7895\n0.8056\n0.7286\n0.6261\n0.6770\n0.5552\n\"\n\"\n%\n75.87\n0.8421\n0.9032\n0.8081\n0.6261\n0.6835\n0.6187\n\"\n\"\n\"\n76.80\n0.8596\n0.9452\n0.8324\n0.6765\n0.6964\n0.6609\nCMR: cross-modal reconstruction; MD: Modality-wise data distillation; MCL: modality-aware contrastive learning\nCross-modal reconstruction. As shown in Table IV, when\nour proposed cross-modal reconstruction is added to the\npre-training, results get a notable performance improvement,\nspecifically from Dice Score 72.52% to 75.16% on the BraTS-\nPED [24]dataset, AUC 0.7719 to 0.8056 on the BraTS2018 [5]\nand ACC 0.5546 to 0.6261 on the ADNI [23] dataset. In ad-\ndition, for the BraTS-PED [24] tumor subregion segmentation\ntask, which has a greater demand for mpMRI information,\nthe incorporation of cross-modal reconstruction leads to the\nprominent improvement in performance, indicating that the\nproposed cross-modal reconstruction can effectively capture\nthe associations between modalities, enabling more efficient\nmulti-modal information fusion.\nModality-wise data distillation. Subsequently, we assess\nthe efficacy of the modality-wise data distillation module.\nImportantly, integrating this module to learn aggregated data\nacross diverse modalities necessitates employing mutual learn-\ning on downstream tasks to establish a linkage between\nupstream and downstream processes. As seen in TableIV,\nthe AUC in the BraTS2018 [5] tumor subtype classifica-\ntion task exhibits a noticeable improvement (from 0.8056\nto 0.9032), suggesting that the distilled modality templates\nobtained through pre-training can effectively enrich the diver-\nsity of downstream data, thereby facilitating our BrainMVP’s\ncapability for generalized representation.\nModality-aware contrastive learning. Finally, we explore\nthe role of modality-aware contrastive learning. It can be\nobserved that with the incorporation of the modality-aware\ncontrastive learning component, the performance of BrainMVP\ncontinues to rise across multiple datasets. On the BraTS-\nPED [24] dataset, the average Dice Score is improved from\n75.87% to 76.80%, and on the BraTS2018 [5] dataset for tu-\nmor subtype classification, the AUC increases from 0.9032 to\n0.9452. On the ADNI [23] dataset, the ACC is enhanced from\n0.6261 to 0.6765. Modality-aware contrastive learning relies\non cross-modal reconstruction and modality-wise data distil-\nlation, and with the integration of these three components, our\nBrainMVP achieves the best results across multiple datasets,\n"
    },
    {
      "page_num": 10,
      "text": "10\nPREPRINT\nFig. 4.\nVisualization of the pre-trained modality templates based on\nmodality-wise data distillation.\ndemonstrating the effectiveness of the proposed BrainMVP\npre-training framework.\nVI. FINAL REMARKS\nIn this paper, we propose an efficient multi-modal vision\npre-training paradigm, BrainMVP, that aligns the features\nacross modalities for multi-parametric brain MRI image anal-\nysis. Inspired by the structural similarities between different\nmodalities of MRI images, we design cross-modal reconstruc-\ntion to learn the correlations between modalities. Meanwhile,\nwe leverage single-channel modality image input for handling\nan arbitrary number of modalities of MRI datasets so as to\nscale up the pre-training datasets. Subsequently, we learn the\ncondensed structural representation of the pre-trained specific\nmodality based on modality-wise data distillation and build\nthe association between pre-training and downstream tasks by\nmixing downstream input modality images with condensed\nmodality templates. In addition, we propose modality-aware\ncontrastive learning to ensure semantic consistency of differ-\nent masking replicas while enhancing the model’s ability to\nextract discriminative information between different samples.\nThrough extensive experiments on ten downstream datasets,\nour proposed BrainMVP demonstrates superior performance\nand strong generalizability across diverse tasks involving mul-\ntiple types of abnormalities. The label efficiency experiment\nshows that we can achieve the performance of the state-of-the-\nart methods using only 40% of the labeled data, demonstrating\nthe potential of BrainMVP in real-world clinical practice.\nREFERENCES\n[1]\nMaruf Adewole et al. “The brain tumor segmentation\n(brats) challenge 2023: Glioma segmentation in sub-\nsaharan africa patient population (brats-africa)”. In:\nArXiv (2023).\n[2]\nJean-Baptiste Alayrac et al. “Flamingo: a visual lan-\nguage model for few-shot learning”. In: Advances\nin neural information processing systems 35 (2022),\npp. 23716–23736.\n[3]\nSaeid Asgari Taghanaki et al. “Deep semantic segmen-\ntation of natural and medical images: a review”. In:\nArtificial Intelligence Review 54 (2021), pp. 137–178.\n[4]\nUjjwal\nBaid\net\nal.\n“The\nrsna-asnr-miccai\nbrats\n2021 benchmark on brain tumor segmentation and\nradiogenomic\nclassification”.\nIn:\narXiv\npreprint\narXiv:2107.02314 (2021).\n[5]\nSpyridon Bakas et al. “Identifying the best machine\nlearning algorithms for brain tumor segmentation,\nprogression assessment, and overall survival predic-\ntion in the BRATS challenge”. In: arXiv preprint\narXiv:1811.02629 (2018).\n[6]\nSpyridon Bakas et al. “Multi-parametric magnetic res-\nonance imaging (mpMRI) scans for de novo Glioblas-\ntoma (GBM) patients from the University of Pennsyl-\nvania Health System (UPENN-GBM)”. In: The Cancer\nImaging Archive (TCIA) Public Access (2021).\n[7]\nEvan Calabrese et al. “The University of California San\nFrancisco preoperative diffuse glioma MRI dataset”. In:\nRadiology: Artificial Intelligence 4.6 (2022), e220058.\n[8]\nXinlei Chen, Saining Xie, and Kaiming He. “An empir-\nical study of training self-supervised vision transform-\ners”. In: Proceedings of the IEEE/CVF international\nconference on computer vision. 2021, pp. 9640–9649.\n[9]\nZekai Chen et al. “Masked image modeling advances\n3d medical image analysis”. In: Proceedings of the\nIEEE/CVF Winter Conference on Applications of Com-\nputer Vision. 2023, pp. 1970–1980.\n[10]\nZhihong Chen et al. “Multi-modal masked autoencoders\nfor medical vision-and-language pre-training”. In: In-\nternational Conference on Medical Image Computing\nand Computer-Assisted Intervention. Springer. 2022,\npp. 679–689.\n[11]\nJunlong Cheng et al. “Sam-med2d”. In: arXiv preprint\narXiv:2308.16184 (2023).\n[12]\nADHD-200 consortium. “The ADHD-200 consortium:\na model to advance the translational potential of neu-\nroimaging in clinical neuroscience”. In: Frontiers in\nsystems neuroscience 6 (2012), p. 62.\n[13]\nAdriana Di Martino et al. “The autism brain imaging\ndata exchange: towards a large-scale evaluation of the\nintrinsic brain architecture in autism”. In: Molecular\npsychiatry 19.6 (2014), pp. 659–667.\n[14]\nYuhang Ding, Xin Yu, and Yi Yang. “RFNet: Region-\naware Fusion Network for Incomplete Multi-Modal\nBrain Tumor Segmentation”. In: Proceedings of the\nIEEE/CVF International Conference on Computer Vi-\nsion. 2021, pp. 3975–3984. (Visited on 02/26/2024).\n[15]\nXiuye Gu et al. “Open-vocabulary object detection via\nvision and language knowledge distillation”. In: arXiv\npreprint arXiv:2104.13921 (2021).\n[16]\nFatemeh Haghighi et al. “Transferable visual words:\nExploiting the semantics of anatomical patterns for self-\nsupervised learning”. In: IEEE transactions on medical\nimaging 40.10 (2021), pp. 2857–2868.\n[17]\nAli Hatamizadeh et al. “Swin unetr: Swin transformers\nfor semantic segmentation of brain tumors in mri im-\nages”. In: International MICCAI Brainlesion Workshop.\nSpringer. 2021, pp. 272–284.\n[18]\nAli Hatamizadeh et al. “Unetr: Transformers for 3d\nmedical image segmentation”. In: Proceedings of the\n"
    },
    {
      "page_num": 11,
      "text": "AUTHOR et al.: TITLE\n11\nIEEE/CVF winter conference on applications of com-\nputer vision. 2022, pp. 574–584.\n[19]\nKaiming He et al. “Masked autoencoders are scalable\nvision learners”. In: Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition.\n2022, pp. 16000–16009.\n[20]\nYuting He et al. “Geometric visual similarity learning\nin 3d medical image self-supervised pre-training”. In:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. 2023, pp. 9538–9547.\n[21]\nMoritz R Hernandez Petzsche et al. “ISLES 2022: A\nmulti-center magnetic resonance imaging stroke lesion\nsegmentation dataset”. In: Scientific data 9.1 (2022),\np. 762.\n[22]\nIXI Dataset. Accessed: July 5, 2024. 2024.\nURL:\nhttps : / / brain - development . org / ixi -\ndataset/.\n[23]\nClifford R Jack Jr et al. “The Alzheimer’s disease\nneuroimaging initiative (ADNI): MRI methods”. In:\nJournal of Magnetic Resonance Imaging: An Official\nJournal of the International Society for Magnetic Res-\nonance in Medicine 27.4 (2008), pp. 685–691.\n[24]\nAnahita Fathi Kazerooni et al. “The brain tumor\nsegmentation (BRATS) challenge 2023: Focus on\npediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI\nBraTS-PEDs)”. In: arXiv preprint arXiv:2305.17033\n(2023).\n[25]\nAishik Konwer et al. “Enhancing Modality-Agnostic\nRepresentations via Meta-Learning for Brain Tumor\nSegmentation”. In: Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision. 2023,\npp. 21415–21425. (Visited on 02/26/2024).\n[26]\nDominic LaBella et al. “The asnr-miccai brain tu-\nmor segmentation (brats) challenge 2023: Intracra-\nnial meningioma”. In: arXiv preprint arXiv:2305.07642\n(2023).\n[27]\nJunnan Li et al. “Align before fuse: Vision and language\nrepresentation learning with momentum distillation”. In:\nAdvances in neural information processing systems 34\n(2021), pp. 9694–9705.\n[28]\nJunnan Li et al. “Blip-2: Bootstrapping language-image\npre-training with frozen image encoders and large lan-\nguage models”. In: International conference on machine\nlearning. PMLR. 2023, pp. 19730–19742.\n[29]\nKunchang Li et al. “Uniformer: Unifying convolution\nand self-attention for visual recognition”. In: IEEE\nTransactions on Pattern Analysis and Machine Intel-\nligence (2023).\n[30]\nLiunian Harold Li et al. “Grounded language-image pre-\ntraining”. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. 2022,\npp. 10965–10975.\n[31]\nHong Liu et al. “M3AE: multimodal representation\nlearning for brain tumor segmentation with missing\nmodalities”. In: Proceedings of the AAAI Conference on\nArtificial Intelligence. Vol. 37. 2. 2023, pp. 1657–1665.\n[32]\nIlya\nLoshchilov\nand\nFrank\nHutter.\n“Decoupled\nweight\ndecay\nregularization”.\nIn:\narXiv\npreprint\narXiv:1711.05101 (2017).\n[33]\nAdri¨enne M Mendrik et al. “MRBrainS challenge: on-\nline evaluation framework for brain image segmentation\nin 3T MRI scans”. In: Computational intelligence and\nneuroscience 2015.1 (2015), p. 813696.\n[34]\nAhmed W Moawad et al. “The brain tumor segmen-\ntation (brats-mets) challenge 2023: Brain metastasis\nsegmentation on pre-treatment mri”. In: ArXiv (2023).\n[35]\nOr Patashnik et al. “Styleclip: Text-driven manipulation\nof stylegan imagery”. In: Proceedings of the IEEE/CVF\ninternational conference on computer vision. 2021,\npp. 2085–2094.\n[36]\nAlec Radford et al. “Learning transferable visual mod-\nels from natural language supervision”. In: Interna-\ntional conference on machine learning. PMLR. 2021,\npp. 8748–8763.\n[37]\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\n“U-net: Convolutional networks for biomedical im-\nage segmentation”. In: Medical image computing and\ncomputer-assisted intervention–MICCAI 2015: 18th in-\nternational conference, Munich, Germany, October 5-9,\n2015, proceedings, part III 18. Springer. 2015, pp. 234–\n241.\n[38]\nJonathan Shapey et al. “Segmentation of vestibular\nschwannoma from MRI, an open annotated dataset\nand baseline algorithm”. In: Scientific Data 8.1 (2021),\np. 286.\n[39]\nJunjie Shi et al. “M2FTrans: Modality-Masked Fusion\nTransformer for Incomplete Multi-Modality Brain Tu-\nmor Segmentation”. In: IEEE Journal of Biomedical\nand Health Informatics (2023).\n[40]\nAron S Talai et al. “Utility of multi-modal MRI for\ndifferentiating of Parkinson’s disease and progressive\nsupranuclear palsy using machine learning”. In: Fron-\ntiers in Neurology 12 (2021), p. 648548.\n[41]\nAiham Taleb et al. “Multimodal self-supervised learning\nfor medical image analysis”. In: International confer-\nence on information processing in medical imaging.\nSpringer. 2021, pp. 661–673.\n[42]\nAiham Taleb et al. “Self-supervised learning for medical\nimages by solving multimodal jigsaw puzzles”. In:\nIeee Transactions on Medical Imaging 12729 (2017),\npp. 661–673.\n[43]\nYucheng Tang et al. “Self-supervised pre-training of\nswin transformers for 3d medical image analysis”. In:\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition. 2022, pp. 20730–20740.\n[44]\nYao-Hung Hubert Tsai et al. “Conditional contrastive\nlearning: Removing undesirable information in self-\nsupervised representations”. In: arXiv e-prints (2021),\narXiv–2106.\n[45]\nJeya Maria Jose Valanarasu et al. “Disruptive Au-\ntoencoders: Leveraging Low-level features for 3D\nMedical\nImage\nPre-training”.\nIn:\narXiv\npreprint\narXiv:2307.16896 (2023).\n"
    },
    {
      "page_num": 12,
      "text": "12\nPREPRINT\n[46]\nFuying Wang et al. “Multi-granularity cross-modal\nalignment for generalized medical visual representation\nlearning”. In: Advances in Neural Information Process-\ning Systems 35 (2022), pp. 33536–33549.\n[47]\nHaoyu Wang et al. “Sam-med3d”. In: arXiv preprint\narXiv:2310.15161 (2023).\n[48]\nHu Wang et al. “Multi-Modal Learning With Missing\nModality via Shared-Specific Feature Modelling”. In:\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition. 2023, pp. 15878–\n15887. (Visited on 02/26/2024).\n[49]\nTongzhou Wang et al. “Dataset distillation”. In: arXiv\npreprint arXiv:1811.10959 (2018).\n[50]\nYiqing Wang et al. “Swinmm: masked multi-view with\nswin transformers for 3d medical image segmentation”.\nIn: International Conference on Medical Image Com-\nputing and Computer-Assisted Intervention. Springer.\n2023, pp. 486–496.\n[51]\nLinshan Wu, Jiaxin Zhuang, and Hao Chen. “VoCo:\nA Simple-yet-Effective Volume Contrastive Learning\nFramework for 3D Medical Image Analysis”. In: arXiv\npreprint arXiv:2402.17300 (2024).\n[52]\nYutong Xie et al. “ReFs: A hybrid pre-training paradigm\nfor 3D medical image segmentation”. In: Medical Image\nAnalysis 91 (2024), p. 103023.\n[53]\nZhenda Xie et al. “Simmim: A simple framework\nfor masked image modeling”. In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition. 2022, pp. 9653–9663.\n[54]\nYiwen Ye et al. “Uniseg: A prompt-driven universal\nsegmentation model as well as a strong representa-\ntion learner”. In: International Conference on Medical\nImage Computing and Computer-Assisted Intervention.\nSpringer. 2023, pp. 508–518.\n[55]\nBo Zhao, Konda Reddy Mopuri, and Hakan Bilen.\n“Dataset condensation with gradient matching”. In:\narXiv preprint arXiv:2006.05929 (2020).\n[56]\nHong-Yu Zhou et al. “A unified visual information\npreservation framework for self-supervised pre-training\nin medical image analysis”. In: IEEE Transactions on\nPattern Analysis and Machine Intelligence (2023).\n[57]\nTongxue Zhou, Su Ruan, and Haigen Hu. “A literature\nsurvey of MR-based brain tumor segmentation with\nmissing modalities”. In: Computerized Medical Imaging\nand Graphics 104 (2023), p. 102167.\n[58]\nYongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba.\n“Dataset distillation using neural feature regression”. In:\nAdvances in Neural Information Processing Systems 35\n(2022), pp. 9813–9827.\n[59]\nZongwei Zhou et al. “Models genesis”. In: Medical\nimage analysis 67 (2021), p. 101840.\n"
    },
    {
      "page_num": 13,
      "text": "AUTHOR et al.: TITLE\n13\nAPPENDIX AND THE USE OF SUPPLEMENTAL FILES\nIn this supplementary material, we provide additional in-\nformation regarding the downstream fine-tuning stage of\nBrainMVP, along with supplementary results for the HD95\nmetric results of six downstream segmentation tasks. Ap-\npendix I provides a detailed explanation of the implementation\ndetails for downstream fine-tuning processes of BrainMVP.\nFig. 5 showcases visual experimental results of BrainMVP and\nother methods on datasets BraTS-PED [24], ISLES22 [21],\nVSseg [38] and MRBrainS13 [33]. Tabel V and Table VI\npresent experimental results for the HD95 metric on six\nsegmentation tasks, comparing BrainMVP with other methods.\nAPPENDIX I\nIMPLEMENTATION DETAILS\nSegmentation. We have built a data augmentation pipeline\nfor segmentation tasks based on the MONAI2 framework. The\ninput mpMRI images are first reoriented to the RAS coordinate\nsystem, then the image spacing is adjusted to a uniform\n1.0mm × 1.0mm × 1.0mm ( for the ISLES22 [21] dataset\nit’s 1.5mm × 1.5mm × 1.5mm ) using bilinear interpolation.\nSubsequently, the pixel grayscale values of the input mpMRI\nimages are normalized from the 5th to the 95th percentile to\na range between 0 and 1 for each channel. After cropping\nthe foreground area of the image, we randomly crop a fixed\narea of 96 × 96 × 96. To avoid over-segmentation, we allow\nthe sampling center to be in the background area. Then, we\nvalidate the input patch for random mirror flipping along\nthree axes with a probability of 0.5. Similarly, we perform\nrandom intensity offset with 0.1 offset and 1.0 probability\nand random intensity scaling with a scale factor of 0.1, also\nwith a probability of 1.0. For network training, we use the\nAdamW [32] optimizer. The initial learning rate is 3e-4 and is\nequipped with cosine learning rate decay, weight decay is 1e-3\nfor UNETR [18] based models, 1e-4 for UniFormer [29] and\nSwin-UNETR [17] based models, and 1e-5 for UNET3D [37]\nbased models. We train the network with a batch size of 3 for\n500 epochs and λcons is set to 0.1.\nClassification. The data augmentation part is different from\nsegmentation in that we resize the input image to a fixed size\nof 128 × 128 × 64 after normalizing it to fit the training of\nthe comparison method ( this generally leads to a decrease in\naccuracy ). Subsequently, we randomly crop a fixed region\nof 96 × 96 × 64 and then perform the same random data\naugmentation as segmentation. In the inference stage, we crop\nan area of 96 × 96 × 64 at the center of the input image. we\nset batch size to 64 based on gradient accumulation and train\nall networks for 200 epochs. The remaining hyper-parameters\nare the same as in segmentation.\n2https://monai.io/\nTABLE V\nEXPERIMENTAL RESULTS ON DATASETS BRATS-PED [24],\nBRATS2023-MET [34] AND ISLES22 [21]. WE REPORT THE MEAN\nHD95 (↓) ON EACH DATASET.\nMethod\nModality\nNetwork\nBraTS-PED [24]\nBraTS2023-MET [34]\nISLES22 [21]\nET\nTC\nWT\nAVG\nET\nTC\nWT\nAVG\nIS\nFrom Scratch\nUNETR [18]\n-\n-\n25.06 39.07 39.14 34.43 44.11 45.22 43.36 44.23\n15.48\nUNET3D [37]\n-\n-\n22.48 34.02 33.07 29.86 45.68 46.85 39.93 44.15\n4.43\nUniFormer [29]\n-\n-\n11.55 16.71 16.14 14.80 25.90 28.16 19.97 24.68\n4.13\nSwin-UNETR [17]\n-\n-\n17.37 22.56 21.03 20.32 28.68 31.03 24.26 27.99\n11.31\nWith General SSL\nMAE3D [19, 9]\nNatural\nUNETR\n25.37 38.43 37.92 33.90 36.89 36.57 38.38 37.28\n15.20\nSimMIM [53]\nNatural\nUNETR\n24.70 31.61 32.52 29.61 39.37 41.26 40.06 40.23\n17.14\nMoCoV3 [8]\nNatural\nUNETR\n20.60 31.88 32.12 28.20 41.88 43.17 41.92 42.32\n15.04\nWith Medical SSL\nMG [59]\nCXR, CT\nUNET3D\n19.71 15.72 17.65 17.69 46.39 48.33 42.02 45.58\n3.68\nTransVW [16]\nCT\nUNET3D\n18.36 25.42 24.67 22.82 47.85 48.06 39.41 45.11\n7.93\nGVSL [20]\nCT\nUNET3D\n17.45 15.33 16.00 16.26 37.33 38.05 30.61 35.33\n9.35\nSwin-UNETR* [43]\nMRI\nSwin-UNETR 18.65 17.44 17.64 17.91 40.57 41.54 33.93 38.68\n8.09\nVoCo [51]\nMRI\nSwin-UNETR 18.98 17.21 17.16 17.78 38.52 39.79 34.73 37.68\n12.22\nDAE [45]\nMRI\nSwin-UNETR 19.33 21.41 21.71 20.82 37.63 37.37 38.74 37.91\n12.50\nM3AE [31]\nMRI\nUNET3D\n13.48 11.91 10.88 12.09 22.40 23.87 18.96 21.74\n4.58\nM3AE [31]\nMRI\nUniFormer\n16.19 15.95 19.78 17.31 25.89 28.37 24.35 26.21\n2.64\nBrainMVP\nMRI\nUNET3D\n15.93\n7.24\n9.81\n10.99 20.37 22.50 18.34 20.40\n5.85\nBrainMVP\nMRI\nUniFormer\n13.93\n7.88\n14.56 12.12 22.60 25.88 19.83 22.77\n2.69\nCXR: Chest X-Ray; ET: enhancing tumor; TC: tumor core; WT: whole tumor; AVG:\naverage; CF: Cerebrospinal Fluid; GM: Gray matter; WM: White matter; IS: Ischemic\nStroke.\nTABLE VI\nEXPERIMENTAL RESULTS ON DATASETS MRBRAINS13 [33],\nVSSEG [38] AND UPENN-GBM [6]. WE REPORT THE MEAN HD95 (↓)\nON EACH DATASET.\nMethod\nModality\nNetwork\nMRBrainS13 [33]\nVSseg [38]\nUPENN-GBM [6]\nCF\nGM\nWM\nAVG\nVS\nET\nTC\nWT\nAVG\nFrom Scratch\nUNETR [18]\n-\n-\n4.16 3.46\n5.04\n4.22\n24.54\n16.97 24.80 31.00 24.26\nUNET3D [37]\n-\n-\n3.24 2.91\n3.70\n3.28\n34.36\n5.30\n9.34\n13.31\n9.32\nUniFormer [29]\n-\n-\n2.38 2.43\n4.04\n2.95\n5.68\n4.46\n6.97\n11.32\n7.58\nSwin-UNETR [17]\n-\n-\n3.38 2.65\n4.00\n3.34\n14.12\n1.86\n7.22\n9.15\n6.08\nWith General SSL\nMAE3D [19, 9]\nNatural\nUNETR\n3.69 2.62\n3.59\n3.30\n24.17\n15.41 20.10 35.71 23.74\nSimMIM [53]\nNatural\nUNETR\n3.84 2.67\n3.55\n3.35\n26.82\n17.23 20.71 32.11 23.35\nMoCoV3 [8]\nNatural\nUNETR\n3.84 2.99\n4.74\n3.86\n21.35\n17.08 19.83 34.35 23.75\nWith Medical SSL\nMG [59]\nCXR, CT\nUNET3D\n3.47 9.43 12.67 8.52\n14.87\n2.27\n4.29\n12.67\n6.41\nTransVW [16]\nCT\nUNET3D\n3.81 3.45\n2.93\n3.40\n16.83\n3.36\n5.73\n12.95\n7.35\nGVSL [20]\nCT\nUNET3D\n3.73 3.44\n3.28\n3.48\n11.58\n2.23\n3.71\n9.17\n5.03\nSwin-UNETR* [43]\nMRI\nSwin-UNETR 3.33 2.26\n2.33\n2.64\n20.73\n2.44\n4.07\n9.79\n5.43\nVoCo [51]\nMRI\nSwin-UNETR 3.14 3.88\n7.87\n4.96\n13.26\n28.50 43.05 31.51 34.35\nDAE [45]\nMRI\nSwin-UNETR 3.07 2.27\n3.36\n2.90\n19.84\n2.24\n3.90\n9.56\n5.23\nM3AE [31]\nMRI\nUNET3D\n3.69 3.88\n3.01\n3.53\n9.20\n1.85\n4.65\n8.24\n4.91\nM3AE [31]\nMRI\nUniFormer\n1.89 2.92\n4.53\n3.11\n9.16\n4.75\n6.54\n9.93\n7.07\nBrainMVP\nMRI\nUNET3D\n3.71 4.92\n3.84\n4.14\n16.41\n2.35\n4.60\n9.13\n5.36\nBrainMVP\nMRI\nUniFormer\n1.53 5.60\n7.02\n4.72\n6.00\n1.48\n6.66\n10.59\n6.24\nCXR: Chest X-Ray; ET: enhancing tumor; TC: tumor core; WT: whole tumor; AVG:\naverage; CF: Cerebrospinal Fluid; GM: Gray matter; WM: White matter; VS: Vestibular\nschwannoma.\n"
    },
    {
      "page_num": 14,
      "text": "14\nPREPRINT\nFig. 5.\nVisualization results of segmentation tasks. (a) BraTS-PED [24]: pediatric tumor subregion segmentation. NCR: necrotic tumor core; ET:\nenhancing tumor. (b) ISLES22 [21]: Ischemic Stroke lesion (IS) segmentation. (c) VSseg [38]: Vestibular schwannoma (VS) segmentation. (d)\nMRBrainS13 [33]: brain structure segmentation. CF: Cerebrospinal Fluid; GM: Gray matter; WM: White matter. GT: ground truth. The green arrows\nhighlight the regions where BrainMVP demonstrates superior performance over other methods.\n"
    }
  ]
}